{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 -  Classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T06:33:05.120070Z",
     "start_time": "2020-05-06T06:33:05.111849Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
    "                             confusion_matrix, classification_report,                              \n",
    "                             precision_recall_curve, roc_curve, roc_auc_score)\n",
    "\n",
    "def load(fname):\n",
    "    mnist = None\n",
    "    try:\n",
    "        with open(fname, 'rb') as f:\n",
    "            mnist = pickle.load(f)\n",
    "            return mnist\n",
    "    except FileNotFoundError:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "        with open(fname, 'wb') as f:\n",
    "            mnist = pickle.dump(mnist, f)\n",
    "        return mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T06:33:05.925535Z",
     "start_time": "2020-05-06T06:33:05.248196Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest\n",
    "mnist_data = load('mnist.data.pkl')\n",
    "X, y = mnist_data['data'], mnist_data['target']\n",
    "# Labelling\n",
    "y_5 = (y=='5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling Methods\n",
    "\n",
    "Resampling methods are indispensible in modern statistics. They involve repeatedly drawing samples from a training set and refitting a model on each sample to obtain additional information about the fitted model.\n",
    "\n",
    "There are generally two ways to do resampling - cross-validation and bootstrapping. Cross-validation can be used to measure the test error associated with a given statistical learning method in order to evaluate its performance or select the appropriate level of flexibility. Bootstrapping is usually used to provide a measure of accuracy of a parameter estimate of a statistical learning method, amongst other uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set\n",
    "\n",
    "In cases where we don't have a large dataset, we split the observations to a training set and a test set. The model is fit on the training set and is evaluated on the test set. The performance of the model is measured by the performance measure on the test set (e.g. MSE, Cross-Entropy loss etc.). \n",
    "\n",
    "In `SKLearn` we use `train_test_split` to achieve this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T06:48:40.937150Z",
     "start_time": "2020-05-06T06:48:39.696337Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T06:51:32.059719Z",
     "start_time": "2020-05-06T06:49:27.846685Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='hinge',\n",
       "              max_iter=1000, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "              power_t=0.5, random_state=0, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "sgd_clf = SGDClassifier(random_state=0)\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T06:52:58.242594Z",
     "start_time": "2020-05-06T06:52:58.206951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict: ['7' '2' '3' '1' '3' '7' '0' '1' '0' '8' '2' '9' '1' '9' '3' '6' '6']\n",
      "Actual : ['7' '8' '7' '1' '3' '7' '0' '8' '0' '8' '2' '9' '1' '9' '3' '6' '6']\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_pred = sgd_clf.predict(X_test)\n",
    "print(\"Predict: \" + str(y_pred[98:115]))\n",
    "print(\"Actual : \" + str(y_test[98:115]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extend, we can randomly draw 15% of observations to be the test set. However, there are limitations to this:\n",
    "1. The validation estimate of test error rate can be highly variable, depending on which observations are used in the validation set.\n",
    "\n",
    "2. Only a subset of observations are used to fit the model. This might overestimate the test error rate for the model fit on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation\n",
    "\n",
    "k-vold cross validation involves splitting the observations into $k$ groups. In the first iteration of model evaluation, the first fold is treated as the validation set and the model is trained on the remaining $k-1$ folds. This is repeated $k$ times, with each successive fold being held out as the validation set. The $k$-fold CV estimate will then be used to average out these values.\n",
    "\n",
    "In `sklearn`, we can perform `k_fold` cross validation using `cross_val_predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T07:00:04.764424Z",
     "start_time": "2020-05-06T07:00:04.761711Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T07:04:03.429576Z",
     "start_time": "2020-05-06T07:00:12.644501Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross_val_predict gives the prediction result of each sample when it is in the test set group\n",
    "sgd_ypred = cross_val_predict(sgd_clf, X_train, y_train, cv=kf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the error rate is simply the mean of the error rates of each of the folds:\n",
    "\n",
    "$$\\text{CV}_{(k)} = \\frac{1}{k}\\sum_{i=1}^K \\text{Err}_i$$ where $\\text{Err}_i = I(\\hat{y}_i \\neq y_i$) or when the value is predicted incorrectly. It is $1$ if the prediction is incorrect and $0$ if the prediction is correct.\n",
    "\n",
    "We can also get scoring values for each fold (say, accuracy) using `cross_val_score`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-06T07:04:37.703814Z",
     "start_time": "2020-05-06T07:04:18.715394Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9325905  0.96470529 0.96435234] 0.9538827107934393 0.015056555299559625\n"
     ]
    }
   ],
   "source": [
    "cvs2 = cross_val_score(estimator=sgd_clf, X=X_train, y=y_train_5, cv=kf, scoring='accuracy')\n",
    "print(cvs2, cvs2.mean(), cvs2.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$k$-fold CV often gives more accurate estimates of the test error rate. This is due to the bias-variance tradeoff.\n",
    "\n",
    "leave-one-out-CV (LOOCV) is where $k=n$ and the number of times to train the model to compute test error rate is $n$ times. Comparing LOOCV and $k$-fold CV, and regular validation set approach, observe that LOOCV provides estimates with the least bias as we train on all the observations, minus 1. A $k$-fold CV has higher bias than LOOCV as each training set contains a smaller proportion of data being left out. \n",
    "\n",
    "But LOOCV also has higher variance then k-fold CV. Because we average the outputs of $n$ fitted models, and are trained on an almost identical set of observations. Hence, these outputs are highly correlated with each other. But k-fold CV uses outputs of models that are someone less correlated because the overlap between each training set is smaller. This lower correlation leads to lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrapping\n",
    "\n",
    "Bootstrapping is used to quantify the uncertainty associated with a given estimator or statistical learning method. It is used by repeatedly sampling observations, with replacement from the orignal dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
