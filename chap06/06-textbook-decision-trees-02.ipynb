{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6 -  Decision Trees\n",
    "\n",
    "### The CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees. First, split the training set using a feature $k$ and a threshold $t_k$. Find this feature-threshold pair $(k,t_k)$ that produces the purest subsets. The cost function to minimise is:\n",
    "\n",
    "$$J(k,t_k) = \\frac{m_{\\text{left}}}{m}G_\\text{left} + \\frac{m_{\\text{right}}}{m}G_\\text{right}$$\n",
    "\n",
    "where $G_{\\text{left or right}}$ is the impurity of the left or right subset and $m_{\\text{left or right}}$ is the number of instances in the left or right subset. $G$ can be either Gini or Entropy.\n",
    "\n",
    "Once it has successfully split the training set in two, it continues to do so recursively until it reaches the max depth or it cannot find a split to further reduce impurity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine whether to use Gini or Entropy, most of the time it does not matter but Gini tends to isolate the most frequent class in its own branch of the tree while entropy leads to more balanced trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:12.867971Z",
     "start_time": "2020-04-16T08:46:11.816008Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from graphviz import Source # For creating the visualisation of the decision tree\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularisation Hypterparameters\n",
    "\n",
    "Decision trees do not need to assume the model is linear. If left unconstrained the tree usually overfits the training data. There are some ways to regularise the model. You can restrict the:\n",
    "- `max_depth` maximum number of traversals from root to leaf\n",
    "- `min_samples_split` minimum number of samples a node must have before a split occurs\n",
    "- `min_samples_leaf` minimum number of samples a leaf node must have\n",
    "- `max_leaf_nodes` maximum number of leaf nodes\n",
    "- `max_features` maximum number of features evaluated for splitting each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "\n",
    "Decision trees are also capable of performing regression tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:12.956489Z",
     "start_time": "2020-04-16T08:46:12.870104Z"
    }
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('housing_X_feateng_complete.csv')\n",
    "y = pd.read_csv('housing_y_feateng_complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:13.029595Z",
     "start_time": "2020-04-16T08:46:12.958614Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=3,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor(max_depth=3)\n",
    "tree_reg.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:13.121853Z",
     "start_time": "2020-04-16T08:46:13.032179Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise tree\n",
    "graph = Source(export_graphviz(tree_reg, out_file=None, \n",
    "                               feature_names=X.columns,))\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of predicting a class, the tree now predicts a value. The prediction is simply an average target value of all the instances in the leaf node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:13.142424Z",
     "start_time": "2020-04-16T08:46:13.126414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74660.69256640242"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = tree_reg.predict(X)\n",
    "np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training, the CART algorithm remains the same, with some tweaks:\n",
    "\n",
    "$$J(k,t_k) = \\frac{m_{\\text{left}}}{m}\\text{MSE}_\\text{left} + \\frac{m_{\\text{right}}}{m}\\text{MSE}_\\text{right}$$\n",
    "\n",
    "where $\\text{MSE}_{\\text{left or right}}$ is the MSE of the left or right subset measured as $\\sum_{i}(\\hat{y}-y^{(i)})^2$ and $\\hat{y} = \\frac{1}{m_{\\text{node}}}\\sum_i y^{(i)}$. Make a prediction using the mean of all instances in the node and calculate MSE from that mean in the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have orgotonal decision boundaries (splits are perpendicular to an axis), which makes the training set sensitive to rotation. One way to help with this is to use PCA. Decision trees are also sensitive to small variations in the data. Random forests can be a way to limit these sensitivities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
