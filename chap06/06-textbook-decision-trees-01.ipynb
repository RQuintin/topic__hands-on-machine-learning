{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 6 -  Decision Trees\n",
    "\n",
    "### Training and Visualising a Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first use the iris dataset as an example. Consider the following tree built for the iris dataset. We use the petal length and petal width to predict which type of iris plant it is. \n",
    "\n",
    "It consists of a series of splitting rules from the top of the tree. For a new sample, we traverse from the top of the tree. In the top split, if the `petal length <= 2.45` it goes to the left branch. The prediction for this example is the Setosa class. If the `petal length > 2.45` then it goes to the right branch. It is further subdivided by the `petal width` feature. Overall, the tree segments the flowers into 3 regions of prediction space. \n",
    "\n",
    "The top node is the root node. This node checks of the petal length is smaller than 2.45cm. The nodes with no more child nodes are terminal nodes or leaf nodes of the tree. The points along the tree are considered internal nodes. The segments of the tree that connects the nodes are branches.\n",
    "\n",
    "Observing the tree, we can say that `petal length` is the most important factor determining type of iris as it is present in the root node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"tree1.png\" width=\"300\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn uses the Classification and Regression Tree (CART) algorithm to train Decision Trees. Given $n$ training examples, each with $p$ features $x_1, \\cdots,x_i,\\cdots, x_n \\in \\mathbb R^p$ and their associated class labels $y_1, \\cdots, y_j, \\cdots, y_n$ where each label is one of $K$ classes i.e. $y_i \\in \\{1,\\cdots, K\\}$, the following steps are taken generally:\n",
    "\n",
    "1. Split the prediction space into $J$ distinct and non-overlapping regions, $R_1, \\cdots, R_m, \\cdots, R_J$. \n",
    "2. For a new observation that falls into $R_m \\in \\{1,\\cdots, J\\}$, the predicted class is the most commonly occuring class in the region (or the proportion of each class in the region).\n",
    "\n",
    "For the Iris example, we have obtained 3 regions, from left $R_1, R_2, R_3$. The response in each of the nodes is $R_1=\\text{setosa}$, $R_2=\\text{versicolor}$, $R_3=\\text{virginica}$ respectively. So for a new observation $x^*$, if $x^* \\in R_1$ then its class prediction is the Setosa.\n",
    "\n",
    "To construct the regions $R_m$, we find regions that maximise class purity. Two ways to measure class purity are Gini impurity or Entropy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a region $R_m$ with $n_{R_m}$observations from each class $k$, Gini impurity / Gini Index is calculated as:\n",
    "$$G_{R_m,\\text{Gini}} = 1-\\sum_{k=1}^{K}p_{R_m,k}^2$$\n",
    "where $p_{R_m,k}$ is the ratio of class $k$ instances among all the training instances in the region $R_m$. It can be also shown that the expression is the same as $\\sum_{k=1}^{K}p_{R_m,k}(1 - p_{R_m,k})$. If all the $p_{R_m,k}$ are close to zero or one, then the Gini index takes on a small value. This means a small value indicates that a node consists predominantly observations from a single class. \n",
    "\n",
    "In the tree context, the region $R_m$ can also mean the node $m$. \n",
    "\n",
    "For a pure node with $[50,0,0]$ members from each class, the Gini score is $1-\\begin{pmatrix}\\frac{0}{50}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{0}{50}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{50}{50}\\end{pmatrix}^2= 0$ \n",
    "\n",
    "For a node of $54$ with $[0,49,5]$ members from each class, the Gini score is $1-\\begin{pmatrix}\\frac{0}\n",
    "{54}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}^2= 0.168$ \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, Entropy or cross-entropy is calculated as:\n",
    "$$G_{R_m,\\text{Entropy}} = -\\sum_{k=1, p_{i,k}\\neq 0}^K p_{R_m,k}\\log p_{R_m,k}$$\n",
    "\n",
    "The bottom term of the summation $p_{i,k}\\neq 0$ means to omit all classes $k$ in the node $i$ with no instances. This is to remove all terms that result in the expression $\\log 0$. The cross entropy is near zero if all the $p_{R_m,k}$ are close to zero or one - hence, similar to the Gini index, the cross entropy score will take a small value if the node is pure.\n",
    "\n",
    "For a pure node with $[50,0,0]$ members from each class, the Entropy calculation is $-\\frac{50}{50} \\log\\begin{pmatrix}\\frac{50}{50}\\end{pmatrix}^2= 0$ \n",
    "\n",
    "For a node of $54$ with $[0,49,5]$ members from each class, the Gini score is $-\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}\\log\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}-\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}\\log\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}= 0.308$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T09:43:38.562680Z",
     "start_time": "2020-04-24T09:43:38.553570Z"
    }
   },
   "source": [
    "\n",
    "First, split the training set using a feature $k$ and a threshold $t_k$. Find this feature-threshold pair $(k,t_k)$ that produces the purest subsets. The cost function to minimise is:\n",
    "\n",
    "$$J(k,t_k) = \\frac{m_{\\text{left}}}{m}G_\\text{left} + \\frac{m_{\\text{right}}}{m}G_\\text{right}$$\n",
    "\n",
    "where $G_{\\text{left or right}}$ is the impurity of the left or right subset and $m_{\\text{left or right}}$ is the number of instances in the left or right subset. $G$ can be either Gini or Entropy.\n",
    "\n",
    "Once it has successfully split the training set in two, it continues to do so recursively until it reaches the max depth or it cannot find a split to further reduce impurity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T08:37:52.215324Z",
     "start_time": "2020-04-24T08:37:51.233318Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "from graphviz import Source # For creating the visualisation of the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T08:37:52.228786Z",
     "start_time": "2020-04-24T08:37:52.218663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest\n",
    "iris_dataset = datasets.load_iris()\n",
    "X = pd.DataFrame(iris_dataset['data'], columns=iris_dataset['feature_names'])\n",
    "y = pd.Series(iris_dataset['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T08:39:59.501198Z",
     "start_time": "2020-04-24T08:39:59.477949Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For testing\n",
    "display(X.head())\n",
    "display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T08:40:27.423779Z",
     "start_time": "2020-04-24T08:40:27.412838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X[['petal length (cm)','petal width (cm)']], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-24T08:40:29.422736Z",
     "start_time": "2020-04-24T08:40:29.356601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Source.gv.pdf'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualise tree\n",
    "graph = Source(export_graphviz(tree_clf, out_file=None, \n",
    "                               feature_names=iris_dataset['feature_names'][2:], class_names=iris_dataset['target_names']))\n",
    "graph.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class purity can be measured by Gini impurity or Entropy. Gini impurity is calculated as:\n",
    "$$G_i = 1-\\sum_{k=1}^np_{i,k}^2$$\n",
    "where $p_{i,k}$ is the ratio of class $k$ instances among all the training instances in the node $i$.\n",
    "\n",
    "For a pure node with $[50,0,0]$ members from each class, the Gini score is $1-\\begin{pmatrix}\\frac{0}{50}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{0}{50}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{50}{50}\\end{pmatrix}^2= 0$ \n",
    "\n",
    "For a node of $54$ with $[0,49,5]$ members from each class, the Gini score is $1-\\begin{pmatrix}\\frac{0}\n",
    "{54}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}^2-\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}^2= 0.168$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is Entropy. It is calculated as \n",
    "$$H_i = -\\sum_{k=1, p_{i,k}\\neq 0}^n p_{i,k}\\log p_{i,k}$$\n",
    "\n",
    "$p_{i,k}\\neq 0$ means to omit all classes $k$ in the node $i$ with no instances.\n",
    "\n",
    "For a pure node with $[50,0,0]$ members from each class, the Entropy calculation is $-\\frac{50}{50} \\log\\begin{pmatrix}\\frac{50}{50}\\end{pmatrix}^2= 0$ \n",
    "\n",
    "For a node of $54$ with $[0,49,5]$ members from each class, the Gini score is $-\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}\\log\\begin{pmatrix}\\frac{49}{54}\\end{pmatrix}-\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}\\log\\begin{pmatrix}\\frac{5}{54}\\end{pmatrix}= 0.308$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:23.249050Z",
     "start_time": "2020-04-16T08:46:23.236583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.1680384087791495\n"
     ]
    }
   ],
   "source": [
    "#Gini Calculations\n",
    "print(1-(0/50)**2-(0/50)**2-(50/50)**2)\n",
    "print(1-(0/54)**2-(49/54)**2-(5/54)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:23.264326Z",
     "start_time": "2020-04-16T08:46:23.254845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0\n",
      "0.30849545083110386\n"
     ]
    }
   ],
   "source": [
    "#Entropy Calculations\n",
    "print(-(50/50)*np.log(50/50))\n",
    "print(-(49/54)*np.log(49/54)-(5/54)*np.log(5/54))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Class Probabilities\n",
    "\n",
    "A decision tree can also estimate the probability that the instance belongs to a particular class $k$. First it traverses the tree to find the leaf node. Then it returns the ratio of training instances of class $k$ for this node. \n",
    "\n",
    "In this case, the number of class 1 for this leaf node is 49 out of 54 so the predicted probability is $\\frac{49}{54}=0.907$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-16T08:46:23.280529Z",
     "start_time": "2020-04-16T08:46:23.268613Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction :  [1]\n",
      "proba :  [[0.         0.90740741 0.09259259]]\n"
     ]
    }
   ],
   "source": [
    "new_s = [[5,1.5]]\n",
    "print('prediction : ', tree_clf.predict(new_s))\n",
    "print('proba : ' , tree_clf.predict_proba(new_s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
