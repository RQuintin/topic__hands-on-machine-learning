{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8 - Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised Learning\n",
    "\n",
    "In supervised learning, we have an access to $p$ features measured on $n$ observations, and a response $y$ is given. The goal is then to predict $y$ using the $p$ features.\n",
    "\n",
    "In unsupervised learning, we only have a set of features $X_1, \\cdots, X_p$ measured on $n$ observations. We are not interested in prediction because we do not have an associated response variable $y$. Rather, the goal is to discover interesting things about the measurements $X_1, \\cdots, X_p$. Can we visualise the data? Can we discover subgroups among the variables or the observations?\n",
    "\n",
    "Unsupervised learning is much more challenging. The analysis tends to be more subjective / biased and there is no simple goal of the analysis. Unsupervised learning is part of <u>exploratory data analysis</u>. Furthermore, in unsupervised learning there is no way to check our work - we don't have tools like cross-validation to measure the performance of our technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Curse of Dimensionality\n",
    "\n",
    "Many ML problems involve training on many features, for each training instance - $p$ can be very large. This process is slow and makes it harder to find a good solution. This is called the curse of dimensionality.\n",
    "\n",
    "Consider the MNIST example. The pixels on the image borders are almost always white (feature has low variation) so they can be removed. Neighbouring pixels usually have the same colour so they can be averaged to form one feature (features have high correlation). Such steps do not result in much information loss.\n",
    "\n",
    "In theory, one solution to overcome the curse of dimsensionality is to increase the size of the training set. However, in reality, the number of training instances required to reach a given density ($\\frac np$) grows exponentially with the number of dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:28.568522Z",
     "start_time": "2020-05-05T07:16:27.204346Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load(fname):\n",
    "    import pickle\n",
    "    mnist = None\n",
    "    try:\n",
    "        with open(fname, 'rb') as f:\n",
    "            mnist = pickle.load(f)\n",
    "            return mnist\n",
    "    except FileNotFoundError:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "        with open(fname, 'wb') as f:\n",
    "            mnist = pickle.dump(mnist, f)\n",
    "        return mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "Consider an ML problem that has a large set of correlated variables (e.g. the neighbouring pixels example in the MNIST dataset). We can summarize this large set of correlated variables with a smaller number of representative variables using principal components.\n",
    "\n",
    "Say we want to visualise $n$ observations with $p$ features $X_1, \\cdots, X_p$. We can visualise the data using $n \\choose 2$ scatterplots. If $p$ is large then we cannot possibly look at all of them. Also, most of them will likely be uninformative as they contain only a small fraction of the total information / variance in the dataset. A better method is to visualise the $n$ observations when $p$ is large. Particularly, we want to find a low-dimensional representation of the data / reduce the dimensions of the data, capturing as much of the information as possible.\n",
    "\n",
    "PCA allows us to do so. The approach is to pick the hyperplane that preserves the most amount of variance, as it will likely lose the least amount of information compared to other projections. Each of these hyperplanes is a <u>linear combination</u> of the $p$ features.\n",
    "\n",
    "The <u>first principal component</u> of a set of features $X_1, \\cdots, X_p$ is the normalised linear combination of the features:\n",
    "\n",
    "$$Z_1 = \\phi_{11}X_1 + \\phi_{21}X_2 + \\cdots + \\phi_{p1}X_p$$\n",
    "\n",
    "that has the largest variance. The elements $\\phi_{j1} \\forall j \\in \\{1\\cdots p\\}$ are the <u>loadings</u> of the first principal component and together, they make the principal component loading vector $\\phi_1$. Mathematically, the first principal component loading vector has the loadings:\n",
    "\n",
    "$$\\phi_1 = \\begin{pmatrix}\\phi_{11}&\\phi_{21}&\\cdots&\\phi_{p1}\\end{pmatrix}^T$$\n",
    "\n",
    "Normalised means that the sum of the loadings $\\sum_{j=1}^p\\phi_{j1}^2=1$. This constraint is needed as setting these elements to be arbitrarily large would results in an arbitrary large variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the first principal components of a $n\\times p$ training set $\\mathbf X$, we first center the data to have mean zero. Then, we find the linear combination of the feature values: \n",
    "\n",
    "$$z_{i1} = \\phi_{11}x_{i1} +  \\phi_{21}x_{i2} + \\cdots +  \\phi_{p1}x_{ip}\\,\\, \\forall i \\in \\{1,\\cdots,n\\}$$\n",
    "\n",
    "that has the largest sample variance subject to the constraint $\\sum_{j=1}^p\\phi_{j1}^2=1$. In other words, the first principal component loading vector solves the optimisation problem:\n",
    "\n",
    "$$\\underset{\\phi_{11}, \\cdots, \\phi_{p1}}{\\text{Maximise }} \\left\\{\\frac 1n \\sum_{i=1}^n\\begin{pmatrix}\\sum_{j=1}^p\\phi_{j1}x_{ij}\\end{pmatrix}^2\\right\\} \\text{ s. t. }$$\n",
    "$$\\sum_{j=1}^p \\phi_{j1}^2=1$$\n",
    "\n",
    "Since $z_{i1} = \\phi_{11}x_{i1} +  \\phi_{21}x_{i2} + \\cdots +  \\phi_{p1}x_{ip}$ we can simplify the optimisation problem to:\n",
    "\n",
    "$$\\underset{\\phi_{11}, \\cdots, \\phi_{p1}}{\\text{Maximise }} \\left\\{\\frac 1n \\sum_{i=1}^nz_{i1}^2\\right\\} \\text{ s. t. }$$ \n",
    "$$\\sum_{j=1}^p \\phi_{j1}^2=1$$\n",
    "\n",
    "Furthermore, since we have a zero-ed mean, that means $\\frac 1n \\sum_{i=1}^nx_{ij}=0$, the mean of $z_{11}, \\cdots, z_{n1}$ is zero as well. Hence, the objective we are maximising is just the sample variance of the $n$ values of $z_{i1}$. We refer $z_{11}, \\cdots, z_{n1}$ as the scores of the first principal component.\n",
    "\n",
    "Solving the optimisation problem involves eigenvalue decomposition. In particular, there is a standard matrix factorization technique called Singlular Value Decomposition (SVD) that decomposes the training set matrix $\\mathbf X$ to the dot product of three matrices:\n",
    "$$\\mathbf X = \\mathbf U \\cdot \\Sigma \\cdot \\mathbf V^T$$ \n",
    "where $\\mathbf V^T$ contains all the principal components that we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Interpretation</b>: The loadings of the first principal component, $\\phi_1$ is the direction in feature space along which the data varies the most. If we project the $n$ training samples onto this direction, the projected values are the principal component scores $z_{11}, \\cdots, z_{n1}$ themselves and they will lose the least amount of information compared to other projections. PCA identifies the axis that accounts for the largest amount of variance in the training set.\n",
    "\n",
    "In this example, the observations are in 2D. The first principal component loading vector is the green line. $\\phi_1 = (\\phi_{11}, \\phi_{21}) = (0.839, 0.544)$\n",
    "<img src=\"0801.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:31.010624Z",
     "start_time": "2020-05-05T07:16:28.570578Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest\n",
    "mnsit = load('mnist.data.pkl')\n",
    "X, y = mnsit['data'], mnsit['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:31.023819Z",
     "start_time": "2020-05-05T07:16:31.014093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    404\n",
      "7    380\n",
      "8    357\n",
      "2    350\n",
      "0    348\n",
      "3    347\n",
      "9    346\n",
      "6    340\n",
      "4    335\n",
      "5    293\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "print(pd.Series(y_test).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:31.053082Z",
     "start_time": "2020-05-05T07:16:31.026876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Center the data\n",
    "X_test_centered = X_test - X_test.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:31.068168Z",
     "start_time": "2020-05-05T07:16:31.057720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 784)\n",
      "(3500, 784)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing\n",
    "print(X_test.shape)\n",
    "print(X_test_centered.shape)\n",
    "X_test_centered[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:34.986988Z",
     "start_time": "2020-05-05T07:16:31.074297Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the principal components using the SVD algorithm\n",
    "u, s, Vt = np.linalg.svd(X_test_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:34.993753Z",
     "start_time": "2020-05-05T07:16:34.989377Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# c1 = Vt.T[:,0] # First PC\n",
    "# c2 = Vt.T[:,1] # Second PC\n",
    "# print(c1)\n",
    "# print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:35.052942Z",
     "start_time": "2020-05-05T07:16:35.003254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3500, 196)\n"
     ]
    }
   ],
   "source": [
    "# Obtain the training set in lower dimensions\n",
    "W2 = Vt.T[:,:196]\n",
    "X2D = X_test_centered.dot(W2)\n",
    "print(X2D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the first principal component $Z_1$ of the features are determined, we can find the second principal component $Z_2$. The second principal component is the linear combination $X_1, \\cdots, X_p$ that has maximal variance out of all linear combinations that are uncorrelated with $Z_1$. The second principal component scores $z_{12}, \\cdots, z_{n2}$ take the form:\n",
    "\n",
    "$$z_{i2} = \\phi_{12}x_{i1} +  \\phi_{22}x_{i2} + \\cdots +  \\phi_{p2}x_{ip}$$\n",
    "\n",
    "where $\\phi_{2}$ is the second principal component loading vector, with elements $\\phi_{12}, \\phi_{22}, \\cdots, \\phi_{p2}$. Note that this loading vector is constrained such that the direction must be orthogonal (perpendicular) to the direction of $\\phi_1$. \n",
    "\n",
    "In 3D space, once we have found $\\phi_1$, there is only one possibility for $\\phi_2$, which is the blue dashed line.\n",
    "\n",
    "<img src=\"0801.png\" width=\"350\" />\n",
    "<img src=\"0803.png\" width=\"600\" />\n",
    "\n",
    "But in a larger dataset with $p>2$ variables, there are multiple candidates for principal components, and they are defined in a similar manner. To find $\\phi_2$, we solve the same maximisation problem, but with the additional constraint that $\\phi_2$ is orthogonal to $\\phi_1$.\n",
    "\n",
    "Once all the principal components are identified, you can reduce the dimensionality of the dataset by projecting it onto the hyperplane defined by the first $d$ principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. To do so, simply compute the dot product of the training sest matrix $\\mathbf X$ by the matrix $\\mathbf W_d$.\n",
    "\n",
    "$$\\mathbf X_{d\\text{-proj}} = \\mathbf X \\cdot \\mathbf W_d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:35.734043Z",
     "start_time": "2020-05-05T07:16:35.056205Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=14**2)\n",
    "X2D_2 = pca.fit_transform(X_test_centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:35.767808Z",
     "start_time": "2020-05-05T07:16:35.736847Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -55.48560463 -208.27434056  223.45570475 ...  159.80138012  541.66155115\n",
      "  600.07788564]\n",
      "\n",
      "[ -55.48560461 -208.27434053  223.45570474 ...  159.80138011  541.66155118\n",
      "  600.07788563]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "print(X2D[:,4])\n",
    "print()\n",
    "print(X2D_2[:,4]*-1.0)\n",
    "print(np.allclose(X2D[:,:], (X2D_2[:,]*-1.0))) # Validate that both are equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of Variance Explained\n",
    "\n",
    "After a projection is complete, we ask how much of the information in a given dataset is lost by projection the observations to the first few principal components? In other words, how much of the variance in the data is not contained in the first few principal components? \n",
    "\n",
    "Generally, we want to find the proportion of variance explained (PVE) by each principal component. The total variance in a dataset is defined as:\n",
    "\n",
    "$$\\sum_{j=1}^p \\text{Var}(X_j) = \\sum_{j=1}^p\\frac 1n \\sum_{i=1}^n x^2_{ij}$$\n",
    "\n",
    "and the variance explained by the $m$th principal component is:\n",
    "\n",
    "$$\\frac 1n \\sum_{i=1}^n z^2_{im} = \\frac 1n \\sum_{i=1}^n\\begin{pmatrix}\\sum_{j=1}^p \\phi_{jm}x_{ij}\\end{pmatrix}^2$$\n",
    "\n",
    "and hence the PVE of the $m$th principal component is:\n",
    "\n",
    "$$\\frac{\\sum_{i=1}^n\\begin{pmatrix}\\sum_{j=1}^p \\phi_{jm}x_{ij}\\end{pmatrix}^2}{\\sum_{j=1}^p \\sum_{i=1}^n x^2_{ij}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PVE of each principal component is a positive quantity. In order to compute the cumulative PVE of the first $M$ principal components, we simply sum the PVE of the expression above.\n",
    "\n",
    "The PVE of each principal component as well as the cumulative PVE can be shown in a scree plot.\n",
    "\n",
    "<img src=\"0804.png\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we aim to use the smallest number of principal components to describe the data aptly. However, there is no good solution to this.\n",
    "\n",
    "Generally, we use a scree plot to help us. Specifically, we look for a point where the proportion of variance explained by each subsequentn principal component drops off. It is referred to as the elbow in the scree plot. In the above example, the elbow after the second principal component. The third principal component captures 10% of the variance and the fourth principal component explains less that 5% and is essentially worthless.\n",
    "\n",
    "Another thing to consider is to choose the number of dimensions that add up to a sufficiently large portion of the variance e.g. 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T07:16:35.788498Z",
     "start_time": "2020-05-05T07:16:35.774685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[333546.90709433 245901.93949278 208021.62030925 181785.38709485\n",
      " 174320.99459052 147759.68427128 111460.35237981 100206.02474743\n",
      "  96227.01303702  84701.88249465  71803.81834356  71369.59851144\n",
      "  60006.8579139   59285.67358456  54027.28339148  52252.30102123\n",
      "  45526.34206775  43809.53252434  40531.50698102  39038.84182736\n",
      "  36614.20697174  35906.63659951  33696.27789464  31729.46761082\n",
      "  29861.70635076  28572.48275191  28181.19662231  26462.09367191\n",
      "  24373.09757148  23662.80238437  22698.8408301   21927.18722011\n",
      "  20804.276075    20103.20944223  19103.77179376  18481.79710125\n",
      "  17863.06071146  17091.44316387  16576.04938153  16074.73288768\n",
      "  15716.4105393   15129.20431514  14265.62620464  13415.91008828\n",
      "  12985.48045643  12673.74288751  12243.44826715  12151.38190587\n",
      "  11624.01388672  11381.53572301  10827.3353768   10698.1021714\n",
      "  10220.64256376   9987.8319534    9746.14321808   9578.48277644\n",
      "   9264.19197474   9036.62671419   8961.46492002   8422.30584433\n",
      "   8320.40753362   8131.57634051   7959.09275556   7596.5501175\n",
      "   7473.03805745   7268.83390436   7015.09038751   6868.46035643\n",
      "   6705.66548802   6552.1033095    6368.12700356   6289.41493804\n",
      "   6220.99696592   5897.45941546   5814.48364824   5651.6595191\n",
      "   5576.31923018   5315.40350624   5147.85406162   5043.6524106\n",
      "   4979.66329333   4801.46999478   4768.38376718   4660.5081187\n",
      "   4567.3663777    4515.38926377   4407.3937108    4291.53591488\n",
      "   4243.1813536    4111.27391535   4090.23711968   4051.41562251\n",
      "   3957.16436808   3841.17568799   3771.8690772    3713.64697535\n",
      "   3627.26030017   3599.61777005   3453.62986507   3410.40003885\n",
      "   3304.52247964   3258.1068736    3156.48308995   3105.47430649\n",
      "   3058.07447628   3043.16454442   2976.25697382   2905.26286943\n",
      "   2835.64445649   2831.53110586   2817.53795788   2776.96041552\n",
      "   2701.66357745   2622.56512961   2612.13056167   2590.47904375\n",
      "   2534.98830673   2521.83089872   2470.33740517   2415.10132245\n",
      "   2366.29621397   2363.16685404   2332.19475785   2286.5554421\n",
      "   2274.2814193    2231.91646368   2172.7649111    2156.07450786\n",
      "   2144.36370553   2078.12002954   2061.15951438   2036.09527725\n",
      "   2020.98357555   1972.87317257   1950.70119261   1944.70215749\n",
      "   1903.64816064   1879.61275297   1835.67790993   1833.41345428\n",
      "   1777.1846972    1751.55515273   1735.68829498   1711.0589397\n",
      "   1690.29425184   1682.12588045   1661.2329773    1638.74351426\n",
      "   1620.07825502   1602.62875575   1582.86576876   1566.10590309\n",
      "   1552.04480089   1527.80294083   1485.77728113   1475.63180011\n",
      "   1453.52730988   1444.67568512   1429.44589109   1406.34720721\n",
      "   1390.63780226   1370.78334151   1367.45274786   1329.87072094\n",
      "   1320.86247367   1316.10498827   1306.97359969   1282.33950976\n",
      "   1268.68646574   1258.2239163    1244.37505884   1223.97347682\n",
      "   1215.53501648   1210.17639397   1206.91379793   1186.27565062\n",
      "   1164.70200128   1144.75605869   1129.5878876    1109.09937913\n",
      "   1101.41274616   1097.78369494   1086.546094     1077.38348876\n",
      "   1058.25548367   1053.29760841   1031.37731925   1014.65642515\n",
      "   1007.60147425    987.06233911    982.22685649    977.03391699\n",
      "    960.62977304    944.79343076    937.88120107    927.5357717 ]\n",
      "[0.09709966 0.07158512 0.06055769 0.05291999 0.05074701 0.04301469\n",
      " 0.0324475  0.02917122 0.02801288 0.02465777 0.02090299 0.02077658\n",
      " 0.01746874 0.0172588  0.01572802 0.0152113  0.01325329 0.0127535\n",
      " 0.01179923 0.01136469 0.01065885 0.01045287 0.00980941 0.00923684\n",
      " 0.00869312 0.00831781 0.0082039  0.00770345 0.00709531 0.00688854\n",
      " 0.00660792 0.00638328 0.00605638 0.00585229 0.00556135 0.00538028\n",
      " 0.00520016 0.00497553 0.00482549 0.00467956 0.00457524 0.0044043\n",
      " 0.0041529  0.00390554 0.00378024 0.00368948 0.00356422 0.00353742\n",
      " 0.0033839  0.00331331 0.00315197 0.00311435 0.00297536 0.00290758\n",
      " 0.00283722 0.00278842 0.00269692 0.00263067 0.00260879 0.00245184\n",
      " 0.00242217 0.0023672  0.00231699 0.00221145 0.00217549 0.00211605\n",
      " 0.00204218 0.00199949 0.0019521  0.0019074  0.00185384 0.00183093\n",
      " 0.00181101 0.00171682 0.00169267 0.00164527 0.00162334 0.00154738\n",
      " 0.0014986  0.00146827 0.00144964 0.00139777 0.00138814 0.00135673\n",
      " 0.00132962 0.00131449 0.00128305 0.00124932 0.00123524 0.00119684\n",
      " 0.00119072 0.00117942 0.00115198 0.00111821 0.00109804 0.00108109\n",
      " 0.00105594 0.00104789 0.00100539 0.00099281 0.00096199 0.00094848\n",
      " 0.00091889 0.00090404 0.00089024 0.0008859  0.00086643 0.00084576\n",
      " 0.00082549 0.00082429 0.00082022 0.00080841 0.00078649 0.00076346\n",
      " 0.00076042 0.00075412 0.00073797 0.00073414 0.00071915 0.00070307\n",
      " 0.00068886 0.00068795 0.00067893 0.00066564 0.00066207 0.00064974\n",
      " 0.00063252 0.00062766 0.00062425 0.00060497 0.00060003 0.00059273\n",
      " 0.00058833 0.00057433 0.00056787 0.00056613 0.00055418 0.00054718\n",
      " 0.00053439 0.00053373 0.00051736 0.0005099  0.00050528 0.00049811\n",
      " 0.00049207 0.00048969 0.00048361 0.00047706 0.00047162 0.00046655\n",
      " 0.00046079 0.00045591 0.00045182 0.00044476 0.00043253 0.00042957\n",
      " 0.00042314 0.00042056 0.00041613 0.00040941 0.00040483 0.00039905\n",
      " 0.00039808 0.00038714 0.00038452 0.00038313 0.00038048 0.00037331\n",
      " 0.00036933 0.00036628 0.00036225 0.00035631 0.00035386 0.0003523\n",
      " 0.00035135 0.00034534 0.00033906 0.00033325 0.00032884 0.00032287\n",
      " 0.00032063 0.00031958 0.00031631 0.00031364 0.00030807 0.00030663\n",
      " 0.00030025 0.00029538 0.00029333 0.00028735 0.00028594 0.00028443\n",
      " 0.00027965 0.00027504 0.00027303 0.00027002]\n"
     ]
    }
   ],
   "source": [
    "print(pca.explained_variance_)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
