{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 8 - Dimensionality Reduction\n",
    "\n",
    "SVMs are very powerful and versatile ML models, able to perform linear or nonlinear classification, regression and outlier detection.\n",
    "\n",
    "### Curse of Dimensionality\n",
    "\n",
    "In most problems, training instances are not spread out uniformly across all dimensions. Some are almost constant while some are highely correlated with each other. Hence, all training instances actually lie within a much lower dimensional subspace of the high dimensional space.\n",
    "\n",
    "Manifold learning relies on the manifuold assumption, that most real-world high dimensional datasets lie close to a much lower-dimensional manifold. E.g. a Swiss Roll is a 2D manifold. It resembles a 2D space but it is bent and we see a 3D space. Also, we assume that the ML task (regression/classification) will be simpler if expressed in the lower dimensional space of the menifold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.145237Z",
     "start_time": "2020-04-21T04:08:01.583748Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.158218Z",
     "start_time": "2020-04-21T04:08:02.149601Z"
    }
   },
   "outputs": [],
   "source": [
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m)*3 * np.pi /2-0.5\n",
    "X = np.empty((m,3))\n",
    "X[:,0] = np.cos(angles) + np.sin(angles)/2 + noise* np.random.randn(m)/2\n",
    "X[:,1] = np.sin(angles) *0.7 + noise * np.random.randn(m)/2\n",
    "X[:,2] = X[:,0]*w1 + X[:,1]*w2 + noise* np.random.randn(m)/2\n",
    "\n",
    "X_centered = X - X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.182108Z",
     "start_time": "2020-04-21T04:08:02.165504Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64719512, -0.50180842, -0.11727884],\n",
       "       [-0.07422696,  0.24952118,  0.06608283],\n",
       "       [-0.44530359,  0.21637711, -0.01268627],\n",
       "       [ 1.1094908 ,  0.14934078,  0.12444768],\n",
       "       [ 0.55256103,  0.44261305,  0.24590183]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For testing\n",
    "X_centered[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "PCA is a very popular dimensionality reduction algorithm. It first identifies the hyperplane that lies closest to the data, then projects the data to it.\n",
    "\n",
    "The idea is to pick the hyperplane that preserves the most amount of variance, as it will likely lose the least amount of information compared to other projections. PCA identifies the axist that accounts for the largest amount of variance in the training set.\n",
    "\n",
    "The unit vector that defines the $i$th axis is called the $i$th principal component (PC). \n",
    "\n",
    "To find the principal components of a training set, there is a standard matrix factorization technique called Singlular Value Decomposition (SVD) that decomposes the training set matrix $\\mathbf X$ to the dot product of three matrices $$\\mathbf U \\cdot \\Sigma \\cdot \\mathbf V^T$$ where $\\mathbf V^T$ contains all the principal components that we are looking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.190251Z",
     "start_time": "2020-04-21T04:08:02.184981Z"
    }
   },
   "outputs": [],
   "source": [
    "u, s, Vt = np.linalg.svd(X_centered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the principal components are identified, you can reduce the dimensionality of the dataset by projecting it onto the hyperplane defined by the first $d$ principal components. Selecting this hyperplane ensures that the projection will preserve as much variance as possible. To do so, simply compute the dot product of the training sest matrix $\\mathbf X$ by the matrix $\\mathbf W_d$.\n",
    "\n",
    "$$\\mathbf X_{d\\text{-proj}} = \\mathbf X \\cdot \\mathbf W_d$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful piece of information is the explained variance ratio of each PC. It indiates the proportion of the dataset's variance that lie along the axis of each PC. \n",
    "\n",
    "Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable ot choose the number of dimensions that add up to a sufficiently large portion of the variance e.g. 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.198246Z",
     "start_time": "2020-04-21T04:08:02.193178Z"
    }
   },
   "outputs": [],
   "source": [
    "c1 = Vt.T[:,0]\n",
    "c2 = Vt.T[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.214260Z",
     "start_time": "2020-04-21T04:08:02.201289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.95865753 -0.23337302 -0.16282746]\n",
      "[-0.27013944  0.92621578  0.26296201]\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.226600Z",
     "start_time": "2020-04-21T04:08:02.220295Z"
    }
   },
   "outputs": [],
   "source": [
    "W2 = Vt.T[:,:2]\n",
    "X2D = X_centered.dot(W2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:02.239775Z",
     "start_time": "2020-04-21T04:08:02.233386Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X2D_2 = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-21T04:08:14.632955Z",
     "start_time": "2020-04-21T04:08:14.626107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.48423371 -0.67045568]\n",
      " [ 0.00216662  0.26853936]\n",
      " [ 0.37846273  0.31736995]\n",
      " [-1.11873732 -0.12867043]\n",
      " [-0.67305031  0.32534951]]\n",
      "\n",
      "[[-0.48423371  0.67045568]\n",
      " [ 0.00216662 -0.26853936]\n",
      " [ 0.37846273 -0.31736995]\n",
      " [-1.11873732  0.12867043]\n",
      " [-0.67305031 -0.32534951]]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# For testing\n",
    "print(X2D[:5])\n",
    "print()\n",
    "print(X2D_2[:5])\n",
    "print(np.allclose(X2D, X2D_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
