{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 -  Training Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression - Validity of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:43:51.042803Z",
     "start_time": "2020-05-11T10:43:49.639680Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:44:04.134565Z",
     "start_time": "2020-05-11T10:44:04.120433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest, preprocessing\n",
    "df = pd.read_csv('Advertising.csv', index_col=0)\n",
    "\n",
    "X1 = df.iloc[:,:1]\n",
    "df1 = df.iloc[:,[0,1,2,3]]\n",
    "\n",
    "X2 = df.iloc[:,:3]\n",
    "df2 = df.iloc[:, [0,3]]\n",
    "y = df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:13:50.081114Z",
     "start_time": "2020-05-11T10:13:50.077927Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# display(df.head())\n",
    "# display(X1.head())\n",
    "# display(df1.head())\n",
    "# display(X2.head())\n",
    "# display(y.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:45:18.191591Z",
     "start_time": "2020-05-11T10:45:18.167572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.612\n",
      "Model:                            OLS   Adj. R-squared:                  0.610\n",
      "Method:                 Least Squares   F-statistic:                     312.1\n",
      "Date:                Mon, 11 May 2020   Prob (F-statistic):           1.47e-42\n",
      "Time:                        18:45:18   Log-Likelihood:                -519.05\n",
      "No. Observations:                 200   AIC:                             1042.\n",
      "Df Residuals:                     198   BIC:                             1049.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.0326      0.458     15.360      0.000       6.130       7.935\n",
      "TV             0.0475      0.003     17.668      0.000       0.042       0.053\n",
      "==============================================================================\n",
      "Omnibus:                        0.531   Durbin-Watson:                   1.935\n",
      "Prob(Omnibus):                  0.767   Jarque-Bera (JB):                0.669\n",
      "Skew:                          -0.089   Prob(JB):                        0.716\n",
      "Kurtosis:                       2.779   Cond. No.                         338.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the coefficients using statsmodels\n",
    "X1 = sm.add_constant(X1)\n",
    "reg11 = sm.OLS(y, X1)\n",
    "results11 = reg11.fit()\n",
    "print(results11.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:29:21.823891Z",
     "start_time": "2020-05-11T10:29:21.819677Z"
    }
   },
   "outputs": [],
   "source": [
    "Xarray2 = np.c_[np.ones((X1.shape[0],1)), X2] # Add x0=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:29:34.720891Z",
     "start_time": "2020-05-11T10:29:34.715242Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.93888937e+00  4.57646455e-02  1.88530017e-01 -1.03749304e-03]\n"
     ]
    }
   ],
   "source": [
    "# For multiple linear regression, obtain the coefficients using the normal equations\n",
    "Theta_hat2 = np.dot(np.dot(np.linalg.inv(np.dot(Xarray2.T, Xarray2)), Xarray2.T),y)\n",
    "print(Theta_hat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T11:56:37.407983Z",
     "start_time": "2020-05-11T11:56:37.387119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     570.3\n",
      "Date:                Mon, 11 May 2020   Prob (F-statistic):           1.58e-96\n",
      "Time:                        19:56:37   Log-Likelihood:                -386.18\n",
      "No. Observations:                 200   AIC:                             780.4\n",
      "Df Residuals:                     196   BIC:                             793.6\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9389      0.312      9.422      0.000       2.324       3.554\n",
      "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
      "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
      "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
      "==============================================================================\n",
      "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
      "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
      "Kurtosis:                       6.332   Cond. No.                         454.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the coefficients using statsmodels\n",
    "X2 = sm.add_constant(X2)\n",
    "reg21 = sm.OLS(y, X2)\n",
    "results21 = reg21.fit()\n",
    "print(results21.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model assumes that the true relationship between $x$ and $y$ is $y=f(x)+\\epsilon$ where $\\epsilon$ is a mean-zero random error term. For univariate linear regression,\n",
    "$$y = f(x) + \\epsilon =\\beta_0 + \\beta_1x_1 + \\epsilon$$ \n",
    "\n",
    "and for multivariate linear regression with $p$ variables, \n",
    "$$y=f(x) + \\epsilon = \\beta_0 + \\beta_1x_{1} + \\beta_2x_{2}+ \\cdots + \\beta_px_{p} + \\epsilon$$ \n",
    "\n",
    "Hence, for the univariate case, $p=1$. \n",
    "\n",
    "Here, $\\beta_0$ is the intercept term - the value of $y$ when $x_j=0 \\,\\,\\forall j \\in \\{1,\\cdots,p\\}$ for the multivariate case. $\\beta_j$ is the average increase in $y$ associated with one unit increase in $x_j$. The error term is a catch-all for what is missed with the model: there may be other variables that cause a variation in $y$, and there may be measurement error. This error term is independent of the $x_j$.\n",
    "\n",
    "This model is the population regression line, and the linear approximation model with parameters $\\hat{\\beta_0}, \\hat{\\beta_1}, \\cdots$ is the least squares line. The true relationship is <u>generally not known</u> and is estimated from the observed data. Fundamentally, we are <u>using observations from an experiment to estimate characteristics of a large population</u>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use the sample mean $\\hat{\\mu}$ to estimate the population mean $\\mu$, we say the estimate is unbiased. On average (across many estimates), we expect $\\hat{\\mu}=\\mu$. Specifically, when we measure $\\hat{\\mu}$ many times and average the estimates, we will get an average that exactly equals $\\mu$. Hence, an unbiased estimator does not systematically overestimate or underestimate the true parameter. This holds for the least squares estimates in this model.\n",
    "\n",
    "If some estimates are above and some are below the true parameter $\\mu$, how, then can we establish how far is a single estimate $\\hat\\mu$ from the true parameter? We use the standard error of $\\hat\\mu$, $\\text{SE}(\\hat\\mu)$ to help us:\n",
    "\n",
    "$$\\text{Var}(\\hat\\mu) = \\text{SE}(\\hat\\mu)^2 = \\frac{\\sigma^2}{n}$$\n",
    "\n",
    "where $\\sigma$ is the population standard deviation. Observe from the formula that the standard error decreases as $n$ increases. The more observations we have in a sample, the smaller the standard error of $\\hat\\mu$. For univariate linear regression, we want to compute the standard errors associated with $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ and they are:\n",
    "\n",
    "$$\\text{SE}(\\beta_0)^2 = \\sigma^2\\begin{bmatrix}\\frac 1n + \\frac{\\bar x^2}{\\sum_{i=1}^n(x-\\bar x)^2}\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{SE}(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x-\\bar x)^2}$$\n",
    "\n",
    "where $\\sigma^2 = \\text{Var}(\\epsilon)$. The assumption is that the errors $\\epsilon_i$ for each observation is uncorrelated and have the same variance $\\sigma^2$. $\\sigma^2$ can be estimated from the data and it is known as the <u>residual standard error</u>, RSE and is given by the formula\n",
    "$$\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}}$$\n",
    "\n",
    "Assuming that the standard errors are Gaussian distributed, Standard errors can be used to compute confidence intervals. a 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined as the lower and upper limits computed from the sample of data. For linear regression, the 95% C.I. for $\\beta_1$ is:\n",
    "\n",
    "$$\\beta_1 \\pm 2\\times \\text{SE}(\\hat{\\beta_1})$$\n",
    "\n",
    "and thus there is a 95% chance that the true value of $\\beta_1$ lies in the interval:\n",
    "\n",
    "$$\\begin{bmatrix}\\beta_1 - 2\\times \\text{SE}(\\hat{\\beta_1}), \\beta_1 + 2\\times \\text{SE}(\\hat{\\beta_1})\\end{bmatrix}$$\n",
    "\n",
    "Similarly, the confidence interval for $\\beta_0$ is:\n",
    "$$\\begin{bmatrix}\\beta_0 - 2\\times \\text{SE}(\\hat{\\beta_0}), \\beta_1 + 2\\times \\text{SE}(\\hat{\\beta_0})\\end{bmatrix}$$\n",
    "\n",
    "(Strictly speaking, the value $2$ in the above equations should be substituted with the 97.5% quantile of a $t$-distribution with $n-2$ degrees of freedom.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Hypothesis Testing I - Univariate Regression</b>\n",
    "\n",
    "Standard errors can also be used to perform hypothesis testing on the coefficients. The null and alternative hypothesis are:\n",
    "$$H_0:\\text{There is no relationship between }x\\text{ and }y$$\n",
    "$$H_1:\\text{There is some relationship between }x\\text{ and }y$$\n",
    "\n",
    "Mathematically, \n",
    "$$H_0:\\beta_1=0$$\n",
    "$$H_1:\\beta_1 \\neq 0$$\n",
    "\n",
    "If the null is true, then the model simply reduces to $y=\\beta_0 + \\epsilon$, with the conclusion that there is no relationship. How large must $\\beta_1$ be to reject the null? It depends on $\\text{SE}(\\hat{\\beta_1})$, relative to (\\hat{\\beta_1}). For a some value of $\\text{SE}(\\hat{\\beta_1})$, the estimate $\\hat \\beta_1$ must be large enough to reject the null hypothesis. To illustrate this, calculate the test statistic, in this case the $t$-statistic:\n",
    "\n",
    "$$t=\\frac{\\hat{\\beta_1}-0}{\\text{SE}(\\beta_1)}$$\n",
    "\n",
    "which measures how many standard deviations is $\\hat \\beta_1$ from $0$. Consequently, the $p$-value is the probability of observing a value larger than or equal to $|t|$, given the null hypothesis is true. \n",
    "\n",
    "A small $p$-value indicates that it is <u>unlikely to observe no relationship</u> between $x$ and $y$, and we can infer that there is indeed a relationship between the variables and the predictor. We reject the null hypothesis and conclude that there is indeed a relationship between the variables and the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T06:49:46.607959Z",
     "start_time": "2020-05-11T06:49:46.602828Z"
    }
   },
   "source": [
    "In the following example:\n",
    "<img src=\"s1.png\" width=\"500\" />\n",
    "<img src=\"s2.png\" width=\"275\" />\n",
    "\n",
    "Observe that the coefficients are large relative to their standard errors. So the $t$-statistics are large and the $p$-values are small. This means the chance of observing $\\beta_0=0$, and $\\beta_1=0$ are extremely small. Hence, we can conclude that $\\beta_0\\neq0$, and $\\beta_1\\neq0$ and there is indeed a relationship between TV and advertising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the Model\n",
    "\n",
    "Now that we have rejected the null hypothesis in favour of the alternative, the next step is to quantify the extent which the model fits the data. This is measured using the residual standard error (RSE) and the $R^2$ statistic. \n",
    "\n",
    "<b>Residual Standard Error (RSE)</b> - Recall that the model contains an error term $\\epsilon$. Because of these error terms, even if we know the true regression line, we cannot perfectly predict $y$ from $x$. The RSE is an estimate of the standard deviation of $\\epsilon$. Roughly, it is the average amount that the response will deviate from the true regression line, and is computed as:\n",
    "\n",
    "$$\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n \\begin{bmatrix} \\hat{y^{(i)}} - y^{(i)})\\end{bmatrix}^2}$$\n",
    "\n",
    "In the example, the RSE is $3.26$. This means actual sales deviate from the true regression line by approximately 3260 units. Or, even if the model were correct, the prediction wll still be off by 3260 on average. How significant this is depends on the ovarall value. If the mean is 14000 units then 3260 will mean an estimation error of about (3260/14000)=23%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>$R^2$ statistic</b> - the RSE provides an absolute measure of lack of fit of the model to the data. But it is measured in units of $y$. To overcome this, we use the $R^2$ statistic, which takes the form of a proportion - the proportion of variance explained, and is independent of $y$. It is calculated as:\n",
    "\n",
    "$$R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$$\n",
    "\n",
    "where $\\text{TSS}$ is defined as total sum of squares, $\\text{TSS} = (y^{(i)}-\\bar y)^2$ and $\\text{RSS} = \\begin{pmatrix} \\hat{y^{(i)}} - y^{(i)}\\end{pmatrix}^2$. TSS is the total variance in the response $y$ and can be thought of the amount of variability before applying the regression model. Hence, $\\text{TSS} - \\text{RSS}$ is the variance explained amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability that can be explained using $x$. An $R^2$ close to $1$ indicates a large proportion of variability is explained by the regression while a value close to $0$ indicates that the regression did not explain much of the variability in the response.\n",
    "\n",
    "In the example, since $R^2=0.612$, about 61% of variability in sales is explained by a linear regression on the TV variable.\n",
    "\n",
    "The $R^2$ is a measure of the linear relationship between $x$ and $y$. Recall that correlation is also a measure of relationship between $x$ and $y$:\n",
    "\n",
    "$$\\text{Cor}(x,y) = r^2=\\frac{\\sum_{i=1}^n (x^{(i)}-\\bar x)\n",
    "(y^{(i)}-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x^{(i)}-\\bar x)^2}\\sqrt{\\sum_{i=1}^n(y^{(i)}-\\bar y)^2}}$$\n",
    "\n",
    "This suggest that we can use correlation to also measure the fit of the linear model. In fact, in univariate linear regression, $r^2 = R^2$. However, this does not apply to the multivariate case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hypothesis Testing II - Multivariate Regression</b>\n",
    "\n",
    "In the multivariate case, we set the null to be that all the coefficients of the regression model are $0$.\n",
    "\n",
    "Mathematically, \n",
    "$$H_0:\\beta_1 = \\beta_2 = \\cdots = \\beta_p =0$$\n",
    "$$H_1:\\text{Any }\\beta_j \\,\\, , j \\in \\{1,\\cdots,p\\} \\text{ is non-zero}$$\n",
    "\n",
    "Now, this is tested using the $F$-statistic:\n",
    "\n",
    "$$F = \\frac{\\frac{\\text{TSS} - \\text{RSS}}{p}}{\\frac{\\text{RSS}}{n-p-1}}$$\n",
    "\n",
    "The definition of TSS and RSS are the same as of univariate regression. If there is no relationship between the response and the variables, then the $F$-statistic is close to $1$. Otherise $F>1$. So $F>1$ is the rule to use to reject the null in favour of the alternative (there is indeed relationship between $x$ and $y$.\n",
    "\n",
    "A large $F$-statistic suggests that at least one of the variables is related to the target variable.\n",
    "\n",
    "Similarly, we can use the $p$-value to determine whether to reject the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T10:59:39.608205Z",
     "start_time": "2020-05-11T10:59:39.588957Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     570.3\n",
      "Date:                Mon, 11 May 2020   Prob (F-statistic):           1.58e-96\n",
      "Time:                        18:59:39   Log-Likelihood:                -386.18\n",
      "No. Observations:                 200   AIC:                             780.4\n",
      "Df Residuals:                     196   BIC:                             793.6\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9389      0.312      9.422      0.000       2.324       3.554\n",
      "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
      "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
      "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
      "==============================================================================\n",
      "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
      "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
      "Kurtosis:                       6.332   Cond. No.                         454.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Obtaining the coefficients using statsmodels\n",
    "X2 = sm.add_constant(X2)\n",
    "reg21 = sm.OLS(y, X2)\n",
    "results21 = reg21.fit()\n",
    "print(results21.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
