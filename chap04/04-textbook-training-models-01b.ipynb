{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 4 -  Training Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:02.921508Z",
     "start_time": "2020-05-14T08:51:01.287955Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:02.938606Z",
     "start_time": "2020-05-14T08:51:02.923976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest, preprocessing\n",
    "df = pd.read_csv('Advertising.csv', index_col=0)\n",
    "\n",
    "X1 = df.iloc[:,:1]\n",
    "df1 = df.iloc[:,[0,1,2,3]]\n",
    "\n",
    "X2 = df.iloc[:,:3]\n",
    "df2 = df.iloc[:, [0,3]]\n",
    "y = df.iloc[:, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:02.946610Z",
     "start_time": "2020-05-14T08:51:02.942833Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# display(df.head())\n",
    "# display(X1.head())\n",
    "# display(df1.head())\n",
    "# display(X2.head())\n",
    "# display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validity of the Coefficient Estimates\n",
    "The linear regression model assumes that the true relationship between $x$ and $y$ is $y=f(x)+\\epsilon$ where $\\epsilon$ is a mean-zero random error term. For univariate linear regression,\n",
    "\n",
    "$$y = f(\\mathbf x) + \\epsilon =\\beta_0 + \\beta_1x_1 + \\epsilon$$ \n",
    "\n",
    "and for multivariate linear regression with $p$ variables, \n",
    "\n",
    "$$y=f(\\mathbf x) + \\epsilon = \\beta_0 + \\beta_1x_{1} + \\beta_2x_{2}+ \\cdots + \\beta_px_{p} + \\epsilon$$ \n",
    "\n",
    "and $p=1$ for the univariate case.\n",
    "\n",
    "$\\beta_0$ is the intercept term - the value of $y$ when $x_j=0 \\,\\,\\forall j \\in \\{1,\\cdots,p\\}$. $\\beta_j$ is the average increase in $y$ associated with one unit increase in $x_j$. The error term, $\\epsilon$ is a catch-all for what is missed with the model: there may be other variables that cause a variation in $y$, and there may be measurement error. This error term is independent of $\\mathbf x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:02.973260Z",
     "start_time": "2020-05-14T08:51:02.950159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.0326      0.458     15.360      0.000       6.130       7.935\n",
      "TV             0.0475      0.003     17.668      0.000       0.042       0.053\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Univariate case\n",
    "X1 = sm.add_constant(X1)\n",
    "uni_reg = sm.OLS(y, X1)\n",
    "univ_results = uni_reg.fit()\n",
    "# display(univ_results.summary2())\n",
    "print(univ_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the univariate case, the model $y = \\beta_0 + \\beta_1x_1 + \\epsilon$ is the <u>population regression line</u>, and the linear approximation model $\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1}x_1$ is the <u>least squares line</u>. In reality, the true relationship between $\\mathbf x$ and $y$ is generally not known, but the parameters of the least squares line can be computed using the closed-form solution using the observed data. Fundamentally, this is the statistical approach where we are <b>using observations from an experiment/sample to estimate characteristics of a large population</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the estimation of the mean problem, we want to find the mean $\\mu_K$ of an unknown variable, say $K$. $\\mu_K$ is unknown but we have $n$ observations from the population and we estimate $\\mu$ using the sample mean $\\bar k = \\sum_{i=1}^n k_i$. The sample mean $\\bar k$ and the population mean $\\mu_K$ are different, but in general the sample mean will provide a good estimate of the population mean.\n",
    "\n",
    "Using $\\hat{\\mu_K} = \\bar k$ to estimate $\\mu_K$ is similar to using $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ to estimate the model parameters $\\beta_0$ and $\\beta_1$. They define the estimated least squares line used to estimate the population regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample mean $\\hat{\\mu_K}=\\bar k$ is an <u>unbiased</u> estimator of the population mean $\\mu_K$. This means that on average (across many estimates of many samples), we expect $\\hat{\\mu_K}=\\mu_K$. Specifically, when we measure $\\hat{\\mu_k}$ many times and average the estimates, we will sometimes overestimate $\\mu_K$ and sometimes underestimate $\\mu_K$. However, by averaging out a large number of estimates, the average will exactly equals $\\mu_K$. \n",
    "\n",
    "Hence, an unbiased estimator does not systematically overestimate or underestimate the true parameter. The property of unbiasedness also holds true for the least squares estimates in the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the estimation of $\\mu_K$ problem, if some estimates $\\hat\\mu_K$ are above and some are below the true population mean, how, then can we establish how far is a single estimate $\\hat\\mu_K$ from the true parameter? We use the <u>standard error</u> of $\\hat\\mu_K$, $\\text{SE}(\\hat\\mu_K)$ to help us:\n",
    "\n",
    "$$\\text{Var}(\\hat\\mu_K) = \\text{SE}(\\hat\\mu_K)^2 = \\frac{\\sigma_K^2}{n}$$\n",
    "\n",
    "where $\\sigma_K$ is the standard deviation of $K$ or the population standard deviation, and $n$ is the sample size. Observe from the formula that the standard error decreases as $n$ increases. The more observations we have in a sample, the smaller the standard error of $\\hat\\mu_K$. \n",
    "\n",
    "In least squares estimation, we want to compute the standard errors associated with $\\hat{\\beta_0}$ and $\\hat{\\beta_1}$ and they are:\n",
    "\n",
    "$$\\text{SE}(\\beta_0)^2 = \\sigma^2\\begin{bmatrix}\\frac 1n + \\frac{\\bar x^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}\\end{bmatrix}$$\n",
    "\n",
    "$$\\text{SE}(\\beta_1)^2 = \\frac{\\sigma^2}{\\sum_{i=1}^n(x_i-\\bar x)^2}$$\n",
    "\n",
    "where $\\sigma^2 = \\text{Var}(\\epsilon)$. The assumption is that the errors $\\epsilon_i$ for each observation is uncorrelated and have the same variance $\\sigma^2$. $\\sigma^2$ can be estimated from the data and it is known as the <u>residual standard error</u>, RSE and is given by the formula\n",
    "\n",
    "$$\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the errors are normally distributed, standard errors can be used to compute <u>confidence intervals</u> (C.I.). a 95% confidence interval is defined as a range of values such that with 95% probability, the range will contain the true unknown value of the parameter. The range is defined as the lower and upper limits computed from the sample of data. In least squares estimation, the 95% C.I. for $\\beta_1$ is:\n",
    "\n",
    "$$\\beta_1 \\pm 1.96\\times \\text{SE}(\\hat{\\beta_1})$$\n",
    "\n",
    "and thus there is a 95% chance that the true value of $\\beta_1$ lies in the interval:\n",
    "\n",
    "$$\\begin{bmatrix}\\beta_1 - 1.96\\times \\text{SE}(\\hat{\\beta_1}), \\beta_1 + 1.96\\times \\text{SE}(\\hat{\\beta_1})\\end{bmatrix}$$\n",
    "\n",
    "Similarly, the 95% C.I. for $\\beta_0$ is:\n",
    "$$\\begin{bmatrix}\\beta_0 - 1.96\\times \\text{SE}(\\hat{\\beta_0}), \\beta_1 + 1.96\\times \\text{SE}(\\hat{\\beta_0})\\end{bmatrix}$$\n",
    "\n",
    "(The value $1.96$ is the corresponding value where $p=0.975$ in the normal table, assuming $n$ is large. Otherwise, use the 97.5% quantile of a $t$-distribution with $n-2$ degrees of freedom.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:02.986307Z",
     "start_time": "2020-05-14T08:51:02.976085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.0326      0.458     15.360      0.000       6.130       7.935\n",
      "TV             0.0475      0.003     17.668      0.000       0.042       0.053\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(univ_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Advertising example, the 95% C.I. for $\\beta_0$ is $[6.130, 7.935]$ and the 95% C.I. for $\\beta_1$ is $[0.042, 0.053]$. Hence, with no advertising, sales will fall on average between 6130 and 7940 units. Each \\$1000 increase in advertising will result in an average increase in sales of between 42 and 53 units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard errors can also be used to perform <u>hypothesis testing</u> on the coefficients. The null and alternative hypothesis are:\n",
    "$$H_0:\\text{There is no relationship between }x\\text{ and }y$$\n",
    "$$H_1:\\text{There is some relationship between }x\\text{ and }y$$\n",
    "\n",
    "Mathematically, \n",
    "$$H_0:\\beta_1=0$$\n",
    "$$H_1:\\beta_1 \\neq 0$$\n",
    "\n",
    "If the null is true, then the model simply reduces to $y=\\beta_0 + \\epsilon$, with the conclusion that there is no relationship. \n",
    "\n",
    "How large must $\\beta_1$ be to reject the null? It depends on $\\text{SE}(\\hat{\\beta_1})$, relative to (\\hat{\\beta_1}). To find the extent of how far $\\beta_1$ is from $0$, we compute the $t$-statistic:\n",
    "\n",
    "$$t=\\frac{\\hat{\\beta_1}-0}{\\text{SE}(\\beta_1)}$$\n",
    "\n",
    "which measures how many standard deviations is $\\hat \\beta_1$ from $0$. Consequently, the $p$-value is the probability of observing a value larger than or equal to $|t|$, given the null hypothesis is true. \n",
    "\n",
    "We can also use the $p$-value to decide whether to reject the null. A small $p$-value indicates that it is <b>unlikely to observe no relationship</b> between $x$ and $y$, and we can infer that there is indeed a relationship between the variables and the predictor. We reject the null hypothesis and conclude that there is indeed a relationship between the variables and the response. Typical thresholds of the $p$-value is $0.05$ or $0.01$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.000538Z",
     "start_time": "2020-05-14T08:51:02.988841Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.0326      0.458     15.360      0.000       6.130       7.935\n",
      "TV             0.0475      0.003     17.668      0.000       0.042       0.053\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "print(univ_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Advertising example, since the $p$-value for both $\\hat \\beta_0$ and $\\hat \\beta_1$ are both very small (virtually $0$), we can reject the null in favour of the alternative: both $\\beta_0 \\neq 0$ and $\\beta_1 \\neq 0$ must be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of the Model\n",
    "\n",
    "Now that we have rejected the null hypothesis in favour of the alternative, the next step is to quantify the extent which the model fits the data. This is measured using the residual standard error (RSE) and <u>the $R^2$ statistic</u>. \n",
    "\n",
    "<b>Residual Standard Error (RSE)</b> - Recall that the model contains an error term $\\epsilon$. Because of these error terms, even if we know the true regression line, we cannot perfectly predict $y$ from $x$. The RSE is an estimate of the standard deviation of $\\epsilon$. Roughly, it is the average amount that the response will deviate from the true regression line, and is computed as:\n",
    "\n",
    "$$\\text{RSE} = \\sqrt{\\frac{\\text{RSS}}{n-2}} = \\sqrt{\\frac{1}{n-2}\\sum_{i=1}^n \\begin{bmatrix} \\hat{y^{(i)}} - y^{(i)})\\end{bmatrix}^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.011293Z",
     "start_time": "2020-05-14T08:51:03.002873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RSE=3.258656368650463\n",
      "mean=14.0225\n"
     ]
    }
   ],
   "source": [
    "print('RSE=' + str(np.sqrt(univ_results.ssr/(univ_results.nobs-2))))\n",
    "print('mean=' + str(y.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Advertising example, the RSE is $3.26$. This means actual sales deviate from the true regression line by approximately 3260 units. Or, even if the model were correct, the prediction wll still be off by 3260 on average. How significant this is depends on the ovarall value. If the mean is 14000 units then 3260 will mean an estimation error of about (3260/14000)=23%.\n",
    "\n",
    "The RSE is a measure of lack of fit of the model to de data. Since it depends on RSS, notice that a small RSE indicates that the model is a good fit of the data, while a large RSE indicates the model does not fit the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>$R^2$ statistic</b> - the RSE provides an absolute measure of lack of fit of the model to the data. But it is measured in units of $y$. To overcome this, we use the $R^2$ statistic, which takes the form of a proportion - the proportion of variance explained, and is independent of $y$. It is calculated as:\n",
    "\n",
    "$$R^2 = \\frac{\\text{TSS} - \\text{RSS}}{\\text{TSS}} = 1 - \\frac{\\text{RSS}}{\\text{TSS}}$$\n",
    "\n",
    "where $\\text{TSS}$ is defined as total sum of squares, $\\text{TSS} = (y^{(i)}-\\bar y)^2$ and $\\text{RSS} = \\begin{pmatrix} \\hat{y^{(i)}} - y^{(i)}\\end{pmatrix}^2$. TSS is the total variance in the response $y$ and can be thought of the amount of variability before applying the regression model. Hence, $\\text{TSS} - \\text{RSS}$ is the variance explained amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability that can be explained using $x$. An $R^2$ close to $1$ indicates a large proportion of variability is explained by the regression while a value close to $0$ indicates that the regression did not explain much of the variability in the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.029661Z",
     "start_time": "2020-05-14T08:51:03.015389Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.612\n",
      "Model:                            OLS   Adj. R-squared:                  0.610\n",
      "Method:                 Least Squares   F-statistic:                     312.1\n",
      "Date:                Thu, 14 May 2020   Prob (F-statistic):           1.47e-42\n",
      "Time:                        16:51:03   Log-Likelihood:                -519.05\n",
      "No. Observations:                 200   AIC:                             1042.\n",
      "Df Residuals:                     198   BIC:                             1049.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "rsquared=0.611875050850071\n"
     ]
    }
   ],
   "source": [
    "print(univ_results.summary().tables[0])\n",
    "print('rsquared=' + str(univ_results.rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the advertising example, since $R^2=0.612$, about 61% of variability in sales is explained by a linear regression on the TV variable.\n",
    "\n",
    "<b>Correlation</b> - The $R^2$ is a measure of the linear relationship between $x$ and $y$. Recall that correlation, $\\text{Cor}(x,y)$ is also a measure of relationship between $x$ and $y$. Correlation is defined as:\n",
    "\n",
    "$$r^2=\\text{Cor}(x,y) = \\frac{\\sum_{i=1}^n (x^{(i)}-\\bar x)\n",
    "(y^{(i)}-\\bar y)}{\\sqrt{\\sum_{i=1}^n (x^{(i)}-\\bar x)^2}\\sqrt{\\sum_{i=1}^n(y^{(i)}-\\bar y)^2}}$$\n",
    "\n",
    "This suggests that we can use $r^2$ to also measure the fit of the univariate linear model. In fact, in the univariate linear model, $r^2 = R^2$. However, this does not apply to the multivariate model, when multiple predictors are used simultaneously to rpredict the response. Hence, since correlation only quantifies the association between <b>one pair of variables</b>, this relationship cannot be extended to the multivariate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.082342Z",
     "start_time": "2020-05-14T08:51:03.033363Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 TV     radio  newspaper     sales\n",
      "TV         1.000000  0.054809   0.056648  0.782224\n",
      "radio      0.054809  1.000000   0.354104  0.576223\n",
      "newspaper  0.056648  0.354104   1.000000  0.228299\n",
      "sales      0.782224  0.576223   0.228299  1.000000\n",
      "\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.0326      0.458     15.360      0.000       6.130       7.935\n",
      "TV             0.0475      0.003     17.668      0.000       0.042       0.053\n",
      "==============================================================================\n",
      "\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         12.3514      0.621     19.876      0.000      11.126      13.577\n",
      "newspaper      0.0547      0.017      3.300      0.001       0.022       0.087\n",
      "==============================================================================\n",
      "\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          9.3116      0.563     16.542      0.000       8.202      10.422\n",
      "radio          0.2025      0.020      9.921      0.000       0.162       0.243\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Correlation plot across \n",
    "print(df.corr())\n",
    "print()\n",
    "print(univ_results.summary().tables[1])\n",
    "print()\n",
    "# Consider univariate linear regression on the other 2 variables\n",
    "X11 = df.iloc[:,[2]]\n",
    "univ_results2 = sm.OLS(y, sm.add_constant(X11)).fit()\n",
    "print(univ_results2.summary().tables[1])\n",
    "print()\n",
    "X12 = df.iloc[:,[1]]\n",
    "univ_results3 = sm.OLS(y, sm.add_constant(X12)).fit()\n",
    "print(univ_results3.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.107255Z",
     "start_time": "2020-05-14T08:51:03.087732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9389      0.312      9.422      0.000       2.324       3.554\n",
      "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
      "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
      "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "# Multivariate case\n",
    "X2 = sm.add_constant(X2)\n",
    "multiv_reg = sm.OLS(y, X2)\n",
    "multiv_results = multiv_reg.fit()\n",
    "print(multiv_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the advertising example, the above table shows the coefficient estimates when TV, radio and newspaper advertising budgets are used simultaneously to predict product sales. For a $1000 increase in radio advertising, holding the rest of the variables constant, we expect to see a 189 unit increase in sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the validity of the 3-variable model against the univariate models? Let the model only for `TV` to be $f_1$, only `newspaper` to be $f_2$, only `radio` to be $f_3$ and the 3-variable model to be $f_4$. Specifically, how does $f_4$ fare against $f_1,f_2$ and $f_3$. Consider the coefficients of the variables. They are relatively similar when comparing $f_4 \\text{vs.} f_1$ for `TV` and $f_4 \\text{vs.} f_3$ for `radio`. However, the coefficient for `newspaper` in $f_2$ is drastically different from that in $f_4$, furthermore, the $p$-value associated with the coefficient is large (0.86).\n",
    "\n",
    "This is because in $f_2$, the coefficient represents the effect of increase on `newspaper` on sales, ignoring the other factors. But in $f_4$, the coefficient represents the effect of increase on `newspaper` on sales <b>holding `TV` and `radio` constant</b>. Does this make sense? Consider the correlation between `newspaper` and `radio` to be large at 0.35. This means there is a tendency to spend more on newspaper where more is spend in radio. \n",
    "\n",
    "If $f_4$ is indeed correct, then in markets where we spend more on radio advertising, our sales tends to be higher, and at the same time we spend more on newspaper advertising. Hence, in $f_2$, although `newspaper` seems to affect sales, we now know that `newspaper` is a surrogate of `radio`. \n",
    "\n",
    "Note how little the regression changes when we fit $f_5$, using only `TV` and `radio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.133917Z",
     "start_time": "2020-05-14T08:51:03.109672Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9211      0.294      9.919      0.000       2.340       3.502\n",
      "TV             0.0458      0.001     32.909      0.000       0.043       0.048\n",
      "radio          0.1880      0.008     23.382      0.000       0.172       0.204\n",
      "==============================================================================\n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9389      0.312      9.422      0.000       2.324       3.554\n",
      "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
      "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
      "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "X13 = df.iloc[:,[0,1]]\n",
    "multiv_results2 = sm.OLS(y, sm.add_constant(X13)).fit()\n",
    "print(multiv_results2.summary().tables[1])\n",
    "print(multiv_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important to note, as when evaluating the soundness of a model, we should not directly link a correlation with a relationship between the variable and predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the relationship between the variables and the response, in the null hyptothesis, we ask whether all the coefficients are indeed 0. This can be validated using hypothesis testing.\n",
    "\n",
    "<b>Hypothesis Testing II - Multivariate Regression</b>\n",
    "\n",
    "In the multivariate case, we set the null hypothesis to be that all the coefficients of the regression model are $0$.\n",
    "\n",
    "Mathematically, \n",
    "$$H_0:\\beta_1 = \\beta_2 = \\cdots = \\beta_p =0$$\n",
    "$$H_1:\\text{Any }\\beta_j \\,\\, , j \\in \\{1,\\cdots,p\\} \\text{ is non-zero}$$\n",
    "\n",
    "Now, this is tested using the $F$-statistic of the regression:\n",
    "\n",
    "$$F = \\frac{\\frac{\\text{TSS} - \\text{RSS}}{p}}{\\frac{\\text{RSS}}{n-p-1}}$$\n",
    "\n",
    "The definition of TSS and RSS are the same as of univariate regression. Using the notation $\\mathbb E(X)$ as the expected value of the random variable $X$, Since \n",
    "$$\\mathbb E(\\frac{\\text{RSS}}{n-p-1})=\\sigma^2$$\n",
    "and if $H_0$ is true,\n",
    "$$\\mathbb E(\\frac{\\text{TSS-RSS}}p)=\\sigma^2$$\n",
    "hence when there is no relationship between the response and the variables, then the $F$-statistic is close to $1$. Otherise$\\mathbb E(\\frac{\\text{TSS-RSS}}p) > \\sigma^2$ and so $F>1$. This is the rule to use to reject the null in favour of the alternative (there is indeed relationship between $x$ and $y$. A large $F$-statistic suggests that at least one of the variables is related to the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.160324Z",
     "start_time": "2020-05-14T08:51:03.136460Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     859.6\n",
      "Date:                Thu, 14 May 2020   Prob (F-statistic):           4.83e-98\n",
      "Time:                        16:51:03   Log-Likelihood:                -386.20\n",
      "No. Observations:                 200   AIC:                             778.4\n",
      "Df Residuals:                     197   BIC:                             788.3\n",
      "Df Model:                           2                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9211      0.294      9.919      0.000       2.340       3.502\n",
      "TV             0.0458      0.001     32.909      0.000       0.043       0.048\n",
      "radio          0.1880      0.008     23.382      0.000       0.172       0.204\n",
      "==============================================================================\n",
      "Omnibus:                       60.022   Durbin-Watson:                   2.081\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              148.679\n",
      "Skew:                          -1.323   Prob(JB):                     5.19e-33\n",
      "Kurtosis:                       6.292   Cond. No.                         425.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                  sales   R-squared:                       0.897\n",
      "Model:                            OLS   Adj. R-squared:                  0.896\n",
      "Method:                 Least Squares   F-statistic:                     570.3\n",
      "Date:                Thu, 14 May 2020   Prob (F-statistic):           1.58e-96\n",
      "Time:                        16:51:03   Log-Likelihood:                -386.18\n",
      "No. Observations:                 200   AIC:                             780.4\n",
      "Df Residuals:                     196   BIC:                             793.6\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          2.9389      0.312      9.422      0.000       2.324       3.554\n",
      "TV             0.0458      0.001     32.809      0.000       0.043       0.049\n",
      "radio          0.1885      0.009     21.893      0.000       0.172       0.206\n",
      "newspaper     -0.0010      0.006     -0.177      0.860      -0.013       0.011\n",
      "==============================================================================\n",
      "Omnibus:                       60.414   Durbin-Watson:                   2.084\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.241\n",
      "Skew:                          -1.327   Prob(JB):                     1.44e-33\n",
      "Kurtosis:                       6.332   Cond. No.                         454.\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "print(multiv_results2.summary())\n",
    "print(multiv_results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that in the advertising example, the $F$- statistic is large for both $f_4$ and $f_5$ so there is strong evidence to reject the null, and conclude there is at least one coefficient is non-zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally, when $n$ is large, an $F$-statistic slightly larger than $1$ might still provide evidence against $H_0$. But a much larger $F$-statistic is needed for to reject $H_0$ for a smaller $n$. The $F$-statistic follows the $F$-distribution and so we can also see the large $F$-statistic corresponds with the small $p$-value. Consequently, we can use the corrsponding $p$-values to decide whether to reject $H_0$.\n",
    "\n",
    "If there are more variables than observations or $p>n$, there are more coefficients to estimate than observartions from which to estimate from. In this case we cannot even fit a least squares model, and consequently the $F$-statistic cannot be used. Other methods like forward selection can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Variable Selection</b> - There can be many combinations of variables to use when doing model selection. For example a 4-variable dataset 6+4+1=11 candidate models (2-var, 3-var and 4-var). The criteria to evaluate models can be Akaike information criteria, Bayesian information criteria (AIC/BIC) or adjusted $R^2$. Grid search is usually not practical with the huge number of candidates. So some approaches are proposed:\n",
    "\n",
    "1. Forward selection starts with a model with no parameters, then successively add 1 parameter at a time that results in the lowest RSS.\n",
    "\n",
    "2. Backward selection starts with a model all parameters, then remove the variable with the largest $p$-value (much like condensing $f_5$ to $f_4$. We could propose to stop when all the $p$-values are below some threshold.\n",
    "\n",
    "3. Mixed selection is a combination of forward and backward selection. Start with the model with no parameters, then add 1 parameter at a time such that all $p$-values are simultaneously below some threshold. If adding 1 variable results in the $p$-value rising above the threshold, it is removed and other variables are tried. Continue until all the variables in the model have a sufficiently low $p$-value and adding any other variables results in the model having a large $p$-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Model Fit</b> - The problem with $R^2$ is that adding another parameter to the model <b>will never decrease</b> $R^2$. Hence, the model with more parameters will always have a better (or equal) fit as measured by $R^2$. This can be seen in the advertising example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:51:03.176983Z",
     "start_time": "2020-05-14T08:51:03.163188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 vars --> 0.897194\n",
      "4 vars --> 0.897211\n"
     ]
    }
   ],
   "source": [
    "print('{:d} vars --> {:.6f}'.format(len(multiv_results2.model.exog[0]), multiv_results2.rsquared))\n",
    "print('{:d} vars --> {:.6f}'.format(len(multiv_results.model.exog[0]), multiv_results.rsquared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T08:30:59.435736Z",
     "start_time": "2020-05-14T08:30:59.418183Z"
    }
   },
   "source": [
    "Recall that \n",
    "\\begin{align*}\n",
    "R^2 = 1- \\frac{\\text{RSS}}{\\text{TSS}} \n",
    "= 1- \\frac{\\begin{pmatrix} \\hat{y^{(i)}} - y^{(i)}\\end{pmatrix}^2}{(y^{(i)}-\\bar y)^2}\n",
    "\\end{align*}\n",
    "\n",
    "By adding 1 more parameter, RSS either stays the same or falls, while TSS remains constant. Resulting in $R^2$ increasing. \n",
    "\n",
    "The inclusion of another variable decreases the <u>degrees of freedom</u> because it requires the estimation of another parameter. Degrees of freedom is described as the excess number of observations, $n$ over the number of parameters (including the intercept), $(p+1)$. Mathematically, $$\\text{df} = n-p-1$$. \n",
    "\n",
    "In the advertising example, adding one more parameter decreases the df from 197 to 196. This decrease has a cost as the lower the degrees of freedom, the less reliable the estimates are likely to be. Hence, the increase in the quality of the fit caused by the addition of a variable needs to be compared to the decrease in the degrees of freedom before a decision can be made with respect to the statistical impact of the added variable.\n",
    "\n",
    "In essence, $R^2$ is of little help if we are trying to decide whether adding a variable to the model improves our ability to meaningfully explain the response. We use $\\bar{R^2}$ which is defined as the percentage of the variation of $Y$ around its mean, $\\bar{Y}$ that is explained by the regression equation, <b>adjusted for degrees of freedom</b>. This is called the <b>adjusted $R^2$</b>. This is always lower than the $R^2$.\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bar{R^2} = 1- \n",
    "\\frac{\n",
    "\t\\frac{\\sum_i\\begin{pmatrix} \\hat{y^{(i)}} - y^{(i)}\\end{pmatrix}^2}{n-p-1}\n",
    "}{\n",
    "\t\\frac{\\sum_i (y^{(i)}-\\bar{y})^2}{n-1}\n",
    "}\n",
    "\\end{equation}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
