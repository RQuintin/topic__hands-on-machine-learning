{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3 -  Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:10:27.632554Z",
     "start_time": "2020-05-07T04:10:26.523814Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import (precision_score, \n",
    "                             recall_score, \n",
    "                             classification_report, \n",
    "                             confusion_matrix, f1_score, \n",
    "                             precision_recall_curve, roc_curve, roc_auc_score)\n",
    "\n",
    "def load(fname):\n",
    "    import pickle\n",
    "    mnist = None\n",
    "    try:\n",
    "        with open(fname, 'rb') as f:\n",
    "            mnist = pickle.load(f)\n",
    "            return mnist\n",
    "    except FileNotFoundError:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "        with open(fname, 'wb') as f:\n",
    "            mnist = pickle.dump(mnist, f)\n",
    "        return mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying with k-Nearest Neighbours (kNN)\n",
    "\n",
    "Given a training set of $n$ observations with $p$ features belonging to one of $c$ classes, we imagine plotting them in $p$-dimensional space. \n",
    "\n",
    "To classify a new example, we first determine the number of nearest neighbours, $k$. We then find the $k$ observations based on a distance measure (e.g. Euclidean distance). Then, we find the \"majority vote\" of classes amongst these neighbours.\n",
    "\n",
    "The psuedo code is as follows:\n",
    "\n",
    "```comments\n",
    "with a new point q\n",
    "for every observation X in 1...n:\n",
    "    calculate the distance between X and q\n",
    "sort the distances in increasing order\n",
    "take k items with lowest distances to q\n",
    "find the majority class among these k items\n",
    "return the majority class as our prediction for the class of q\n",
    "```\n",
    "\n",
    "Note that for kNN, since we use Euclidean distance, if one feature has a much larger range than another, then it will dominate the distance calculation (influence the distance calculation much more). Hence, ideally normalise the values before performing the distance calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:14:05.472239Z",
     "start_time": "2020-05-07T04:14:03.463626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest\n",
    "mnist_data = load('mnist.data.pkl')\n",
    "X, y = mnist_data['data'], mnist_data['target']\n",
    "y = y.astype(int)\n",
    "\n",
    "# Limit classification to only 1000 samples\n",
    "df = pd.DataFrame(X)\n",
    "df['y'] = y\n",
    "df_samples = []\n",
    "for i, j in df.groupby('y'):\n",
    "    df_samples.append(j.sample(1200).copy())\n",
    "df_new = pd.concat(df_samples)\n",
    "X_new = df_new.iloc[:,:784].copy()\n",
    "y_new = df_new['y'].copy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, test_size=0.15, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, predict the correct cleaned image from the noisy image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:14:10.320750Z",
     "start_time": "2020-05-07T04:14:09.405868Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a kNN classifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:14:25.565893Z",
     "start_time": "2020-05-07T04:14:24.938876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 9, 9, 6, 9, 4, 7, 5, 9, 8, 6, 7, 0, 3, 8, 7, 8, 3, 0, 8, 3, 0, 5, 1, 6]\n",
      "[4, 9, 9, 6, 9, 4, 7, 5, 9, 8, 6, 7, 0, 3, 8, 7, 8, 3, 0, 8, 3, 0, 5, 1, 6]\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "y_predict = knn_clf.predict(X_test[:50])\n",
    "print(y_test[:25].tolist())\n",
    "print(y_predict[:25].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T04:15:43.049626Z",
     "start_time": "2020-05-07T04:14:28.721051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.93647059 0.94088235 0.93852941]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(knn_clf, X_train, y_train, scoring='accuracy', cv=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying with Probability Theory: Naïve Bayes\n",
    "\n",
    "<b>Conditional Probability</b> - Recall that for 2 events $X$ and $C$ with probabilities $P(X)$ and $P(C)$, the joint probability of both events occuring is $P(X,C)$. Take note that $P(X,C) = P(C,X)$.\n",
    "\n",
    "The conditional probability of $C$ occuring given $X$, $P(C|X)$ and is evaluated as $P(C|X) = \\frac{P(C,X)}{P(X)}$. Since we also know $P(X|C) = \\frac{P(C,X)}{P(C)} \\rightarrow P(C,X) = P(X|C)\\cdot {P(C)}$, we arrive at the conditional probability statement:\n",
    "\n",
    "$$P(C|X) = \\frac{P(X|C)\\cdot P(C)}{P(X)}$$\n",
    "\n",
    "#### Naïve Bayes Model\n",
    "Bayes' rule extends the conditional probability statement. For a new observation $\\mathbf x = (x_1, \\cdots, x_p)^T$, what is the probability that it belongs to one of the $K$ classes? Mathematically, we extend the above statment now to form $P(c_k|\\mathbf x)$ and the statement is now:\n",
    "\n",
    "$$P(c_k|\\mathbf x) = \\frac{P(\\mathbf x | c_k)\\cdot P(c_k)}{P(\\mathbf x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Assumptions </b> - Naïve Bayes assumes independence of every features in $\\mathbf x$ for $j \\in \\{1,\\cdots,p\\}$. In the document classification problem, if the vocabulary of the corpus is $p$, then statistical independence means that the presence of each word is independent of the presence of another word. This is unlikely to be true but this assumption holds for the model. The model also assumes that every feature is equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Training</b> - In the document classification problem, there are $N$ documents in the training set, belonging to one of $K$ classes and the vocabulary of the corpus is $P$. The number of $\\mathbf x_i$ is a vector representing the document, with each element in the vector representing the count of words in the vocabulary. So ${x}_i = (x_{i1}, \\cdots ,x_{ij},\\cdots, x_{iP})^T$, If the third word in the vocabulary is `bacon`, and it occurs twice in the first document, then $x_{13}=2$.\n",
    "\n",
    "$P(c_k)$ is simply the number of documents in the class $k$, divided by $N$. $$P(c_k) = \\frac{\\sum_{i=1}^N I(c_i=k)}{N}$$\n",
    "\n",
    "If there are 100 documents and 15 of them belong to $k=1$, then $P(c_1) = \\frac{15}{100}=0.15$\n",
    "\n",
    "Consider the term $P(\\mathbf x_i | c_k)$. It can be expanded to $P(x_{11},x_{12},\\cdots,x_{1p} | c_k)$. Which is the joint probability of all the words in the document for that class. Since the occurence of every word is independent of each other, we can rewrite the above expression to:\n",
    "$$P(x_{i1},x_{i2},\\cdots,x_{ip} | c_k) = P(x_{i1}|c_k)\\cdot P(x_{i2}|c_k)\\cdot \\cdots P(x_{ip}|c_k)$$\n",
    "\n",
    "and this helps with our calculations. $P(x_{i1}|c_k)$ is simply the count of the word $w_1$ amongst all words in class $c_k$.\n",
    "\n",
    "To classify a new document, we calclulate $P(c_k|\\mathbf x)$ for every class in $K$ and select the class with the largest posterior probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T03:54:16.679855Z",
     "start_time": "2020-05-07T03:54:16.365738Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ingest\n",
    "ng = fetch_20newsgroups()\n",
    "Xng, yng = ng['data'], ng['target']\n",
    "X_train_ng, X_test_ng, y_train_ng, y_test_ng = train_test_split(Xng, yng, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T03:54:19.654059Z",
     "start_time": "2020-05-07T03:54:16.682076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform to vector, and train\n",
    "vec = TfidfVectorizer()\n",
    "Xng_matrix = vec.fit_transform(X_train_ng)\n",
    "\n",
    "nb_clf = MultinomialNB()\n",
    "nb_clf.fit(Xng_matrix, y_train_ng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T03:54:20.364860Z",
     "start_time": "2020-05-07T03:54:19.657025Z"
    }
   },
   "outputs": [],
   "source": [
    "# Transform to vector, and predict\n",
    "Xng_test_matrix = vec.transform(X_test_ng)\n",
    "y_predict_ng = nb_clf.predict(Xng_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T03:54:20.372244Z",
     "start_time": "2020-05-07T03:54:20.366886Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 12 13 14  9  9 11  8 14 11 10  9  4  9  0  9 13 14  7  5]\n",
      "[ 1  1 13 14  9  9 11  8 14 11  9  9  4  9  0  9 13 14 17  5]\n"
     ]
    }
   ],
   "source": [
    "# for testing\n",
    "print(y_test_ng[:20])\n",
    "print(y_predict_ng[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T03:56:08.705370Z",
     "start_time": "2020-05-07T03:56:08.198042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.83031374 0.82501105 0.84047724 0.83377542]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(nb_clf, Xng_matrix, y_train_ng, scoring='accuracy', cv=4))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
