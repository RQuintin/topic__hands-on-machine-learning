{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 - Support Vector Machines\n",
    "\n",
    "### Nonlinear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier is used when the boundary between the two classes are linear. However, in practice, we are sometimes faced with non-linear boundaries. In this case, we consider enlarging the feature space using the higher order features. E.g. rather than fitting a support vector classifier on $p$ features $\\begin{pmatrix} X_1, \\cdots, X_p\\end{pmatrix}$, we add a polynomial (squared) feature and fit the support vector classifier on $2p$ features $\\begin{pmatrix} X_1, X_1^2 \\cdots, X_p, X_p^2\\end{pmatrix}$. Now, the optimisation problem will be:\n",
    "\n",
    "$$\\underset{\\beta_0, \\beta_{11}, \\beta_{12}, \\cdots, \\beta_{p1},\\beta_{p2},\\epsilon_1, \\cdots, \\epsilon_n}{\\text{Maximise }}M \\text{ s. t. }$$\n",
    "$$\\sum_{j=1}^p\\sum_{k=1}^2 \\beta_{jk}^2=1$$\n",
    "$$y_i\\begin{pmatrix}\\beta_0 + \\sum_{j=1}^p\\beta_{j1}x_{ij} + \\sum_{j=1}^p\\beta_{j2}^2x^2_{ij} \\end{pmatrix}\\geq M(1-\\epsilon_i)\\,\\,\\forall i \\in \\{1,\\cdots,n\\}$$\n",
    "$$\\epsilon_i \\geq 0\\,\\,\\forall i \\in \\{1,\\cdots,n\\}\\,\\,, \\sum_{i=1}^n \\epsilon_i \\leq C$$\n",
    "\n",
    "In this enlarged feature space, the decision boundary is linear. However, in the original future space, the decision boundary is in the form $q(x)=0$ where $q$ is a quadratic polynomial, adn its solutions are generally non-linear. In extension, we can enlarge the feature space with higher polynomial terms or interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.700338Z",
     "start_time": "2020-04-28T09:37:25.374283Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import (make_moons, load_iris)\n",
    "from sklearn.preprocessing import (PolynomialFeatures, StandardScaler)\n",
    "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this in SKLearn, use `PolynomialFeatures` to transform before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.713245Z",
     "start_time": "2020-04-28T09:37:26.702691Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.744654Z",
     "start_time": "2020-04-28T09:37:26.719334Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='hinge', max_iter=1000000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform to 3rd degree polynomial features\n",
    "polyfeatures1 = PolynomialFeatures(degree=3)\n",
    "scaler1 = StandardScaler()\n",
    "X_expt1 = polyfeatures1.fit_transform(X_train)\n",
    "X_expt1 = scaler1.fit_transform(X_expt1)\n",
    "\n",
    "# Train on polynomial features\n",
    "clf_expt1 = LinearSVC(C=10, loss='hinge', max_iter=1000000)\n",
    "clf_expt1.fit(X_expt1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not hard to see that there are endless ways to enlarge the feature space and can come up with many features. This computationally becomes unmanageable. The support vector machine allows us to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification - Using Kernels\n",
    "\n",
    "The Support Vector Machine extends the support vector classifier that results from <u>enlarging the feature space using kernels</u>. This results in a method that is more efficient computationally.\n",
    "\n",
    "The following from SKLearn implements this using a 3rd degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.759300Z",
     "start_time": "2020-04-28T09:37:26.748592Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=1,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 3rd degree polynomial and then train SVC on it\n",
    "clf_expt12= SVC(kernel='poly', degree=3, coef0=1, C=10)\n",
    "clf_expt12.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution to the SVC problem involves only the <u>inner products</u> of the observations (as opposed to the observations themselves). The inner product of two vectors $\\vec{a}$ and $\\vec{b}$ of length $r$, $\\langle\\,\\vec{a},\\vec{b}\\rangle$ is $\\sum_{i=1}^ra_ib_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.768845Z",
     "start_time": "2020-04-28T09:37:26.762627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "a, b = np.array([3,4]), np.array([2,7])\n",
    "print(np.dot(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner product of two observations $x_i,x_{i'}$, denoted by $\\langle x_{i},x_{i'}\\rangle$ is hence\n",
    "$$\\langle x_{i},x_{i'}\\rangle = \\sum_{j=1}^p x_{ij}x_{i'j}$$\n",
    "\n",
    "The linear SVC solution can then be represented as:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum_{i=1}^n \\alpha_i \\langle x,x_{i}\\rangle$$\n",
    "\n",
    "where there are $n$ parameters, $\\alpha_i \\forall i \\in \\{1,\\cdots,n\\}$\n",
    "\n",
    "To estimate the parameters $\\alpha_i, i \\in \\{1,\\cdots,n\\}$ and $\\beta_0$, we need the inner products $\\langle x,x_{i}\\rangle$ of all the pairs of training observations.\n",
    "\n",
    "It turns out that $\\alpha_i$ is nonzero for only the support vectors in the solution. So if the training observation is not the support vector then $\\alpha_i=0$. So if $S$ is the collection of indices of these support points, we can rewrite the solution function in the form:\n",
    "\n",
    "$$f(x) = \\beta_0 + \\sum_{i\\in S}^n \\alpha_i \\langle x,x_{i}\\rangle$$ which involve far fewer terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To expand the solution, instead of using the inner product $\\langle x,x_{i}\\rangle$ we use a generalisation of the inner product in the form: $$K(x_i,x_{i'})$$\n",
    "\n",
    "where $K$ is some function we refer to as a <u>kernel</u>. A kernel is a function that quantifies the similarity of two observations. If $K_{\\text{linear}}(x_i,x_{i'}) = \\sum_{j=1}^p x_{ij}x_{i'j}$ then $K_{\\text{linear}}$ is a linear kernel and the solution returns the support vector classifier. \n",
    "\n",
    "Instead, if the kernel is now $K_{\\text{polynomial}}(x_i,x_{i'}) = (1 + \\sum_{j=1}^p x_{ij}x_{i'j})^d$ where $d$ is a positive integer then we have $K_{\\text{polynomial}}$ the polynomial kernel of degree $d$ \n",
    "\n",
    "Using a nonlinear kernel to perform classification results in the support vector machine. Now, the solution has the form:\n",
    "\n",
    "$$\\begin{align}f(x) &= \\beta_0 + \\sum_{i\\in S}^n \\alpha_i K_{\\text{polynomial}}(x_i,x_{i'})\\\\&=\\beta_0 + \\sum_{i\\in S}^n \\alpha_i (1 + \\sum_{j=1}^p x_{ij}x_{i'j})^d\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular choice is the radial kernel, $K_{\\text{radial}}(x_i,x_{i'}) = \\exp(-\\gamma \\sum_{j=1}^p (x_{ij}-x_{i'j})^2)$ where $\\gamma$ is a positive constant. Naturally, the solution to the radial kernel has the form:\n",
    "\n",
    "$$\\begin{align}f(x) &= \\beta_0 + \\sum_{i\\in S}^n \\alpha_i K_{\\text{radial}}(x_i,x_{i'})\\\\&=\\beta_0 + \\exp(-\\gamma \\sum_{j=1}^p (x_{ij}-x_{i'j})^2)\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.785958Z",
     "start_time": "2020-04-28T09:37:26.772077Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=0.001, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma=5, kernel='rbf', max_iter=-1,\n",
       "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature Scaling\n",
    "scaler2 = StandardScaler()\n",
    "X_expt3 = scaler1.fit_transform(X_train)\n",
    "\n",
    "# Train on RBF Kernel\n",
    "clf_expt13 = SVC(kernel='rbf', gamma=5, C=0.001)\n",
    "clf_expt13.fit(X_expt3, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the radial kernel work? Given an unseen test observation $x^*$ is far from a training observation $x_i$ in terms of Euclidean distance, then $\\sum_{j=1}^p (x^*_{j}-x_{ij})^2)$ is large and hence $K_{\\text{radial}}(x^*,x_{i})$ is small. Then $x_i$ will play no role in $f(x^*)$. Training observations far from $x^*$ will play no role in the prediction for the class $x^*$\n",
    "\n",
    "This means the radial kernel has very local behaviour, where only nearby training observations have an effect on the class label of the test observation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The advantage of using kernels  is computational. When using kernels, we only compute $K$ for every pair of the observations. This can be done without working explicitly working on an enlarged feature space. In enlarged feature space solutions, the feature space could be so large the computations are intractable. In the case of the radial kernel, the feature space is implicit and infinite-dimensional, so we cannot perform the mathematical calculations anyway."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Regression (SVR)\n",
    "\n",
    "The SVM algorithm is also versatile: It can support linear and nonlinear regression. The idea is to reverse the objective: instead of finding the largest possible street between two classes, we now find as many instances as possible on the street. The width of the street is controlled by a parameter $\\epsilon$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.803805Z",
     "start_time": "2020-04-28T09:37:26.788277Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVR(C=1.0, dual=True, epsilon=1.5, fit_intercept=True,\n",
       "          intercept_scaling=1.0, loss='epsilon_insensitive', max_iter=1000,\n",
       "          random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear SVR, no polynomial features\n",
    "svr1 = LinearSVR(epsilon=1.5)\n",
    "svr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, apply the polynomial kernel transformation if we want a SVM with a polynomial (not linear) solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T09:37:26.964031Z",
     "start_time": "2020-04-28T09:37:26.809026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, cache_size=200, coef0=0.0, degree=2, epsilon=0.1, gamma='scale',\n",
       "    kernel='poly', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVR, with polynomial features\n",
    "svr2 = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n",
    "svr2.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
