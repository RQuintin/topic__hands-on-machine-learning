{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5 - Support Vector Machines\n",
    "\n",
    "### Nonlinear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The support vector classifier is used when the boundary between the two classes are linear. However, in practice, we are sometimes faced with non-linear boundaries. In this case, we consider enlarging the feature space using the higher order features. E.g. rather than fitting a support vector classifier on $p$ features $\\begin{pmatrix} X_1, \\cdots, X_p\\end{pmatrix}$, we add a polynomial (squared) feature and fit the support vector classifier on $2p$ features $\\begin{pmatrix} X_1, X_1^2 \\cdots, X_p, X_p^2\\end{pmatrix}$. Now, the optimisation problem will be:\n",
    "\n",
    "$$\\underset{\\beta_0, \\beta_{11}, \\beta_{12}, \\cdots, \\beta_{p1},\\beta_{p2},\\epsilon_1, \\cdots, \\epsilon_n}{\\text{Maximise }}M \\text{ s. t. }$$\n",
    "$$\\sum_{j=1}^p\\sum_{k=1}^2 \\beta_{jk}^2=1$$\n",
    "$$y_i\\begin{pmatrix}\\beta_0 + \\sum_{j=1}^p\\beta_{j1}x_{ij} + \\sum_{j=1}^p\\beta_{j2}^2x^2_{ij} \\end{pmatrix}\\geq M(1-\\epsilon_i)\\,\\,\\forall i \\in \\{1,\\cdots,n\\}$$\n",
    "$$\\epsilon_i \\geq 0\\,\\,\\forall i \\in \\{1,\\cdots,n\\}\\,\\,, \\sum_{i=1}^n \\epsilon_i \\leq C$$\n",
    "\n",
    "In this enlarged feature space, the decision boundary is linear. However, in the original future space, the decision boundary is in the form $q(x)=0$ where $q$ is a quadratic polynomial, adn its solutions are generally non-linear. In extension, we can enlarge the feature space with higher polynomial terms or interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:24.959718Z",
     "start_time": "2020-04-28T03:05:24.028274Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import (make_moons, load_iris)\n",
    "from sklearn.preprocessing import (PolynomialFeatures, StandardScaler)\n",
    "from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve this in SKLearn, use `PolynomialFeatures` to transform before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:24.970257Z",
     "start_time": "2020-04-28T03:05:24.961886Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:24.994141Z",
     "start_time": "2020-04-28T03:05:24.974626Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=10, class_weight=None, dual=True, fit_intercept=True,\n",
       "          intercept_scaling=1, loss='hinge', max_iter=1000000,\n",
       "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "          verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform to 3rd degree polynomial features\n",
    "polyfeatures1 = PolynomialFeatures(degree=3)\n",
    "scaler1 = StandardScaler()\n",
    "X_expt1 = polyfeatures1.fit_transform(X_train)\n",
    "X_expt1 = scaler1.fit_transform(X_expt1)\n",
    "\n",
    "# Train on polynomial features\n",
    "clf_expt1 = LinearSVC(C=10, loss='hinge', max_iter=1000000)\n",
    "clf_expt1.fit(X_expt1, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not hard to see that there are endless ways to enlarge the feature space and can come up with many features. This computationally becomes unmanageable. The support vector machine allows us to enlarge the feature space used by the support vector classifier in a way that leads to efficient computations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinear SVM Classification - Using Kernels\n",
    "\n",
    "The Support Vector Machine extends the support vector classifier that results from <u>enlarging the feature space using kernels</u>. This results in a method that is more efficient computationally.\n",
    "\n",
    "The following from SKLearn implements this using a 3rd degree polynomial kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:25.008478Z",
     "start_time": "2020-04-28T03:05:24.997117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=1,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 3rd degree polynomial and then train SVC on it\n",
    "clf_expt12= SVC(kernel='poly', degree=3, coef0=1, C=10)\n",
    "clf_expt12.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:25.017742Z",
     "start_time": "2020-04-28T03:05:25.011220Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Using the RBF kernel\n",
    "# clf4 = SVC(kernel='rbf', gamma=5, C=0.001)\n",
    "# X_expt3_feats = scl2.fit_transform(X_train)\n",
    "# clf4.fit(X_expt3_feats, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SV Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:25.032406Z",
     "start_time": "2020-04-28T03:05:25.024744Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Linear SVR, no polynomial features\n",
    "# svr1 = LinearSVR(epsilon=1.5)\n",
    "# svr1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-28T03:05:25.045148Z",
     "start_time": "2020-04-28T03:05:25.038481Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Linear SVR, with polynomial features\n",
    "# svr2 = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n",
    "# svr2.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
