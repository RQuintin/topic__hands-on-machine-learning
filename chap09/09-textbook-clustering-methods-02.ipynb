{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 9 - Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In $k$-means clustering, we need to specify the number of clusters $k$ before running the algorithm ($k$ is thought to be the hyperparameter of this algorithm). <u>Hierarchical clustering</u> is an alternative approach which does not require us to commit to a particular choice of $k$. Also, hierarchical clustering results in an easy-to-interpret tree-based representation of the observations called a <u>dendogram</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting a Dendogram\n",
    "\n",
    "Consider a dataset with 45 observations in a 2-dimensional space. We aim to perform hierarchical clustering of the data. The following is the scatterplot of the observations.\n",
    "\n",
    "The following is how the dataset is visualised in a dendogram. Each observation is represented as a leaf at the bottom of the tree. As we traverse up the tree, leaves fuse to form a branch. As we continue to traverse up, branches & leaves fuse further. Observations that fuse at the lower levels of the tree are quite similar while observations that fuse at the higher levels of the tree are quite different. Specifically, the height where the observation fuses, measure by the position on the $y$-axis, indicates how different the two observations are. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify clusters using the dendogram, we make a horizontal cut across the dendogram The observations that belong to each branch is interpreted as one cluster. \n",
    "\n",
    "Observe that when we cut at position 9, we obtain two clusters, while if we cut at position 5, we get three clusters. As we cut at a lower position, we obtain more clusters. In other words, the height to cut the dendogram serves the same role as determining $k$ in $k-means clustering.\n",
    "\n",
    "Hierarchical clustering is named as the clusters obtained by cutting the dendogram at a given height are nested within the clusters obtained by cutting the dendogram at a greater height. (Using this example, the clusters obtained at height 5 will fuse to a smaller number of clusters obtained at height 9). However, the assumption of a hierarchical structher might be unrealistic. Clusters could be created by slicing and dicing the data across features (e.g. gender, followed by race). Hence, sometimes hierarchical clustering might yield worse results than $k$-means clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Hierarchical Clustering Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Dissimilarity Measure"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
