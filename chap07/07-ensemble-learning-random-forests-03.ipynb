{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Ensemble Learning and Random Forests\n",
    "\n",
    "### Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is an Ensemble method that combines several weak learners to create a strong learner. The general idea is to train predictors sequentially, each trying to correct its predecessor. \n",
    "\n",
    "The most popular boosting methods are Adaboost and Gradient Boosting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:52:20.194658Z",
     "start_time": "2020-04-17T08:52:20.190379Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way for a new predictor to correct its predecessor is to pay more attention to the training instances that the predecessor underfitted. This results in predictors focusong more and more of the wrongly classified cases. This is the technique used by Adaboost.\n",
    "\n",
    "To build an Adaptive Boosting (Adaboost) classifier, a base classifier is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained using the updated weights and makes predictions on the training set, and so on.\n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, but predictors that have different weights depending on their overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:48:58.639428Z",
     "start_time": "2020-04-17T08:48:58.629805Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:48:59.007513Z",
     "start_time": "2020-04-17T08:48:58.643492Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for Adaboost classifier\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, \n",
    "                            learning_rate=0.5, algorithm='SAMME.R')\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weight is applied to every example in the training data. Initially, they are all equal so $w^{(i)}=\\frac 1m$. A first predictor is trained. The errors are calculated using the error rate $r$.\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1,\\hat{y}^{(i)} \\neq {y}^{(i)}}^M w^{(i)}}{\\sum_{i=1}^M w^{(i)}}$$\n",
    "The numerator is sum of weights of all incorrectly classified instances while the denominator is sum of weights of all instances.\n",
    "\n",
    "The predictor's weight $\\alpha$ is computed using $\\eta \\log \\frac{1-r}{r}$ where $\\eta$ is the learning rate.\n",
    "\n",
    "Now, a second predictor is trained on the training set again. But the weights of the training set are adjusted so the examples correctly classified have a smaller weight and those that were wrongly classified have a larger weight. To do so, \n",
    "\n",
    "$$w_\\text{new} = \\begin{cases}\\frac{w_\\text{old} \\exp(\\alpha)}{\\sum_i w^{(i)}} \\text{ if classified correctly or }\\hat{y}^{(i)} = {y}^{(i)}\\\\\\frac{w_\\text{old} \\exp(-\\alpha)}{\\sum_i w^{(i)}} \\text{ if classified incorrectly or }\\hat{y}^{(i)} \\neq {y}^{(i)}\\end{cases}$$\n",
    "\n",
    "\n",
    "With multiple predictors and weights, the predicted class is the one that receives the maximum score of the weighted votes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:39.092897Z",
     "start_time": "2020-04-17T06:56:38.336307Z"
    }
   },
   "source": [
    "For gradient boosted trees, it also addes predictors sequentially to the ensemble. However, instead of tweaking the weights of the instance, the method fits the new predcitor to the residual errors made by the previous predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T08:53:00.148787Z",
     "start_time": "2020-04-17T08:53:00.138219Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for Adaboost classifier\n",
    "gbt_clf = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbt_clf.fit(X_train, y_train)\n",
    "y_pred = gbt_clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
