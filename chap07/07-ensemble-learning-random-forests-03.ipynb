{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Ensemble Learning and Random Forests\n",
    "\n",
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that bagging creates multiple copies of the original training set by performing bootstrap sampling. Each tree is built on a bootstrap dataset, independent of the other trees. <u>Boosting</u> is similar but each tree is grown sequentially: each tree is grown using information from previously grown trees. \n",
    "\n",
    "The most popular methods for boosting are Adaboost and Gradient Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:19.986014Z",
     "start_time": "2020-04-27T09:16:18.973871Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (BaggingClassifier, RandomForestClassifier, \n",
    "                              ExtraTreesClassifier, AdaBoostClassifier, GradientBoostingClassifier)\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:19.997118Z",
     "start_time": "2020-04-27T09:16:19.988363Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost\n",
    "\n",
    "To build an Adaptive Boosting (Adaboost) classifier, first determine the number of predictors, $B$. A base classifier $\\hat{f^1}(x)$, say a Decision Tree model is trained using the training set. Then, $\\hat{f^1}(x)$ is used to make predictions on the training set. Naturally, there will be correctly predicted instances and wrongly predicted instances. The relative weight of misclassified training instances is then increased. \n",
    "\n",
    "A second classifier, $\\hat{f^2}(x)$, another decision tree model is trained using the training set <u> but with the updated weights</u>. We can see that $\\hat{f^2}(x)$ will perform better at the misclassified training instances due to $\\hat{f^1}(x)$. It now is used to make predictions on the training set, the relative weights of misclassified training instances is increased again, and a third classifier $\\hat{f^3}(x)$ is trained, and so on and so forth until $\\hat{f^B}(x)$, the last classifier.\n",
    "\n",
    "In building these $B$ classifiers, we can see that each new predictor corrects its predecessor by increasing the weights, and hence paying more attention to the training instances that its predecessor underfitted. This results in predictors focusong more and more of the wrongly classified cases.\n",
    "\n",
    "The boosting approach learns slowly. We fit a tree using the current residuals, rather than the outcome $y$ as the response. We then add this new treed into the fitted function in order to update the residuals. By fitting small trees to the residuals, we slowly improve $\\hat f$ in ares where it does not perform well. \n",
    "\n",
    "Generally, boosting has three parameters:\n",
    "\n",
    "- The number of trees $B$. Note that boosting can overfit if $B$ is too large. Generally, $B$ is selected using cross-validation\n",
    "\n",
    "- The learning rate $\\lambda$. This controls the rate at which boosting learns. Typical values are small like 0.01 or 0.001.\n",
    "\n",
    "- The number of splits of each tree, $d$. \n",
    "\n",
    "Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, but predictors that have different weights depending on their overall accuracy on the weighted training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.355998Z",
     "start_time": "2020-04-27T09:16:20.003390Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for Adaboost classifier\n",
    "# B is n_estimators, \n",
    "# lambda is learning_rate\n",
    "# d is number of layers, indicating number of splits\n",
    "ada_clf = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1), n_estimators=200, \n",
    "                            learning_rate=0.5, algorithm='SAMME.R')\n",
    "ada_clf.fit(X_train, y_train)\n",
    "y_pred = ada_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A weight is applied to every example in the training data. Initially, they are all equal so $w^{(i)}=\\frac 1m$. A first predictor is trained. The errors are calculated using the error rate $r$.\n",
    "\n",
    "$$r = \\frac{\\sum_{i=1,\\hat{y}^{(i)} \\neq {y}^{(i)}}^M w^{(i)}}{\\sum_{i=1}^M w^{(i)}}$$\n",
    "The numerator is sum of weights of all incorrectly classified instances while the denominator is sum of weights of all instances.\n",
    "\n",
    "The predictor's weight $\\alpha$ is computed using $\\eta \\log \\frac{1-r}{r}$ where $\\eta$ is the learning rate.\n",
    "\n",
    "Now, a second predictor is trained on the training set again. But the weights of the training set are adjusted so the examples correctly classified have a smaller weight and those that were wrongly classified have a larger weight. To do so, \n",
    "\n",
    "$$w_\\text{new} = \\begin{cases}\\frac{w_\\text{old} \\exp(\\alpha)}{\\sum_i w^{(i)}} \\text{ if classified correctly or }\\hat{y}^{(i)} = {y}^{(i)}\\\\\\frac{w_\\text{old} \\exp(-\\alpha)}{\\sum_i w^{(i)}} \\text{ if classified incorrectly or }\\hat{y}^{(i)} \\neq {y}^{(i)}\\end{cases}$$\n",
    "\n",
    "\n",
    "With multiple predictors and weights, the predicted class is the one that receives the maximum score of the weighted votes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:39.092897Z",
     "start_time": "2020-04-17T06:56:38.336307Z"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient boosting, like Adaboost, works by adding predictors sequentially to the ensemble. However, instead of tweaking the weights of the instance, the method fits the new predcitor to the residual errors made by the previous predictor. Generally the algorithm is as follows:\n",
    "\n",
    "Given the training data $X$ and the target values $y$, \n",
    "1. Determine the number of trees to build, $B$. Set residuals $r_i = y_i$ for the training set.\n",
    "2. Repeatedly fit a tree $\\hat{f^b}$ to the training data $(X, r)$. Add the new tree to the boosted model: $f(x) \\leftarrow f(x) + \\lambda \\hat{f^b}(x)$. Update the residuals $r_i \\leftarrow r_i - \\lambda \\hat{f^b}(x_i)$\n",
    "3. Add this tree in step 2 to the overall boosted model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.372537Z",
     "start_time": "2020-04-27T09:16:20.358318Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B = 3\n",
    "# Fit f1\n",
    "tree1 = DecisionTreeClassifier(max_depth=2)\n",
    "tree1.fit(X_train, y_train)\n",
    "yres1 = y_train - tree1.predict(X_train)\n",
    "\n",
    "# Fit f2\n",
    "tree2 = DecisionTreeClassifier(max_depth=2)\n",
    "tree2.fit(X_train, yres1)\n",
    "yres2 = yres1 - tree2.predict(X_train)\n",
    "\n",
    "# Fit f2\n",
    "tree3 = DecisionTreeClassifier(max_depth=2)\n",
    "tree3.fit(X_train, yres2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.381033Z",
     "start_time": "2020-04-27T09:16:20.377220Z"
    }
   },
   "outputs": [],
   "source": [
    "# For testing\n",
    "# print(tree1.predict(X_test[:1]))\n",
    "# print(tree2.predict(X_test[:1]))\n",
    "# print(tree3.predict(X_test[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.392777Z",
     "start_time": "2020-04-27T09:16:20.385833Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n"
     ]
    }
   ],
   "source": [
    "# Predict using the boosted model\n",
    "y_pred = sum([tree.predict(X_test[:1]) for tree in [tree1, tree2, tree3]])/len([tree1, tree2, tree3])\n",
    "print([1 if y_pred>0.5 else 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting learns slowly. In each step, the tree fits on the residuals rather than the outcome. This new decison tree is added to the fitted function to update the residuals. By fitting small trees, we slowly improve the ensemble model in areas that does not perform well. The shrinkage parameter / learning rate slows the process down event further, allowing other trees to shave off from the residuals. \n",
    "\n",
    "The following is SKLearn's implementation of the gradient boosting algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.407095Z",
     "start_time": "2020-04-27T09:16:20.395362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for GradientBoosting classifier\n",
    "gbt_clf = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "gbt_clf.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = gbt_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.528717Z",
     "start_time": "2020-04-27T09:16:20.410299Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for GradientBoosting classifier\n",
    "gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=120, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "# Predict\n",
    "y_pred = gbrt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the optimal number of trees, you can use early stopping. A simple way to see this is to use `staged_predict()`. It returns an iterator over the predictions made by the ensemble at each stage of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.578665Z",
     "start_time": "2020-04-27T09:16:20.530869Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.472, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104, 0.104]\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                           learning_rate=0.1, loss='deviance', max_depth=2,\n",
       "                           max_features=None, max_leaf_nodes=None,\n",
       "                           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                           min_samples_leaf=1, min_samples_split=2,\n",
       "                           min_weight_fraction_leaf=0.0, n_estimators=1,\n",
       "                           n_iter_no_change=None, presort='deprecated',\n",
       "                           random_state=None, subsample=1.0, tol=0.0001,\n",
       "                           validation_fraction=0.1, verbose=0,\n",
       "                           warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# After every tree is built, find the MSE of the predictions of that particular ensemble.\n",
    "errors = [mean_squared_error(y_test, y_pred_i) for y_pred_i in gbrt.staged_predict(X_test)]\n",
    "print(errors[:30])\n",
    "\n",
    "# Of the errors, find the trees that have made the minimum MSE.\n",
    "best_n_estimators = np.argmin(errors)\n",
    "\n",
    "# This means that best_n_estimators is 22. The best ensemble contains 22 trees.\n",
    "print(best_n_estimators)\n",
    "\n",
    "# Then use this number of trees as the best classifier.\n",
    "gbrt_best = GradientBoostingClassifier(max_depth=2, n_estimators=best_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.588417Z",
     "start_time": "2020-04-27T09:16:20.582206Z"
    }
   },
   "outputs": [],
   "source": [
    "errors_df = pd.DataFrame({'i' : range(len(errors)), 'e' : errors})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.850885Z",
     "start_time": "2020-04-27T09:16:20.591491Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting for different alpha values\n",
    "fig, ax = plt.subplots(figsize=(10,4))\n",
    "errors_df.plot(kind='line', x='i', y='e', ax=ax, color='maroon')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way is to actually stop early:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T09:16:20.894036Z",
     "start_time": "2020-04-27T09:16:20.853828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.168\n",
      "if\n",
      "0.168\n",
      "else\n",
      "0.12\n",
      "if\n",
      "0.136\n",
      "else\n",
      "0.128\n",
      "else\n",
      "0.12\n",
      "else\n",
      "0.128\n",
      "else\n",
      "0.12\n",
      "else\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# warm_start=True keeps existing trained trees after fit() is called\n",
    "# subsample=0.25 means use only 25% of the training instances, sampled randomly\n",
    "gbrt_growing = GradientBoostingClassifier(max_depth=2, warm_start=True, subsample=0.25) \n",
    "min_val_error = float('inf')\n",
    "\n",
    "error_going_up = 0\n",
    "\n",
    "# Stop training when the validation error does not improve for 5 consecutive iterations\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt_growing.n_estimators = n_estimators\n",
    "    gbrt_growing.fit(X_train, y_train)\n",
    "    y_pred = gbrt_growing.predict(X_test)\n",
    "    validation_error = mean_squared_error(y_test, y_pred)\n",
    "    print(validation_error)\n",
    "    if validation_error < min_val_error:\n",
    "        print('if')\n",
    "        min_val_error = validation_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        print('else')\n",
    "        error_going_up +=1\n",
    "        if error_going_up == 5:\n",
    "            break\n",
    "            \n",
    "print(n_estimators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
