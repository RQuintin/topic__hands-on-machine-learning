{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Ensemble Learning and Random Forests\n",
    "\n",
    "### Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get diverse classifiers is to use different models. Another way is to use the same model but train it on different subsets of the training set. \n",
    "\n",
    "Training a model can be done on randomly drawn subsets of the training set. If drawing is with replacement it is called bagging or bootstrapping. If drawing is without replacement it is called pasting.\n",
    "\n",
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors (e.g. using a hard voting classifier).  Aggregating predictors reduce both bias and variance. Also, these different predictors can be trained in parallel. This is also why they are preferred - they scale very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:36.379910Z",
     "start_time": "2020-04-17T06:56:35.865459Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:36.390581Z",
     "start_time": "2020-04-17T06:56:36.382507Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:38.333293Z",
     "start_time": "2020-04-17T06:56:36.394291Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for bagging classifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, \n",
    "                            max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out-of-bag instances can be used to perform evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:39.092897Z",
     "start_time": "2020-04-17T06:56:38.336307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n",
      "[[0.35449735 0.64550265]\n",
      " [0.43684211 0.56315789]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Train for bagging classifier using OOB\n",
    "bag_clf2 = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, \n",
    "                            n_jobs=-1, oob_score=True)\n",
    "bag_clf2.fit(X_train, y_train)\n",
    "# Obtain the oob score\n",
    "print(bag_clf2.oob_score_)\n",
    "# Obtain the decision function for each OOB sample\n",
    "print(bag_clf2.oob_decision_function_[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:39.344366Z",
     "start_time": "2020-04-17T06:56:39.095313Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.928\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for the test set\n",
    "y_pred2 = bag_clf2.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of this, you can sample features too. Then each predictor will be trained on a random subset of the input features. This uses the `max_features` param.\n",
    "\n",
    "Sampling both features and instances is Random Patches, while sampling only features is Random Subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random forests are more optimised for Decision Trees. Also, instead of searching for the best feature to split a node, it searches for the best feature among a random subset of features. This gives greater tree diversity which trades higher bias for lower variance, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:40.544662Z",
     "start_time": "2020-04-17T06:56:39.346436Z"
    }
   },
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "y_pred3 = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features are likely to appear closer to the root of the tree. It is possible to see this by computing the average depth that the feature appears in all the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:40.662641Z",
     "start_time": "2020-04-17T06:56:40.546653Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41746298, 0.58253702])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra trees further enhances the Random forest by searching random thresholds for each feature rather than searching for the best threshold to split on. This is an Extremely Randomised Trees ensemble (or Extra Trees ensemble). This is also much faster to train as finding the best threshold is the most expensive step of growing a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-17T06:56:41.558126Z",
     "start_time": "2020-04-17T06:56:40.667332Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrees_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "xtrees_clf.fit(X_train, y_train)\n",
    "y_pred4 = xtrees_clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
