{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 7 - Ensemble Learning and Random Forests\n",
    "\n",
    "### Bagging and Pasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to get diverse classifiers is to use different models. Another way is to use the same model but train it on different subsets of the training set. \n",
    "\n",
    "Decision trees generally suffer from high variance. If we train the same model on different subsets of the training set, we could get vastly different results.\n",
    "\n",
    "<b>Bagging</b>, or bootstrap aggregation is a general purpose procedure to reduce the variance of a statistical learning method. Recall that given a set of $n$ independent observations, $Z_1, \\cdots, Z_n$, each with variance $\\sigma^2$, the variance of the mean $\\bar Z$ of the observations is given by $\\frac{sigma^2}{n}$. In other words, averaging a set of observations reduces variance. \n",
    "\n",
    "Bagging involves generating $B$ different bootstrapped training datasets. We then train a model on each bootstrapped training set in order to get $\\hat f ^{*b}(x)$ and average all the predictions to get \n",
    "$$\\hat f _{\\text{bag}}(x) = \\frac 1B \\sum _{b=1}^B \\hat f ^{*b}(x)$$\n",
    "\n",
    "Bagging is particularly useful for decision trees. To apply bagging on regression trees, construct $B$ regression trees using $B$ bootstrapped training sets and average the resulting predictions. Each tree has high variance, but low bias. Averaging the prediction of $B$ trees reduces variance. In contrast, to apply bagging for classification, we can use hard voting (the outcome of the ensemble is the majority vote of all the learners) or soft voting (predict the class with the highest class probability, averaged across all the learners).\n",
    "\n",
    "Also, these different predictors can be trained in parallel. This is also why they are preferred - they scale very well.\n",
    "\n",
    "If each training dataset is created by drawing with replacement it is called bagging or bootstrapping. If the drawing is without replacement it is called pasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:05.560077Z",
     "start_time": "2020-04-27T07:51:04.508225Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "\n",
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:05.571787Z",
     "start_time": "2020-04-27T07:51:05.562249Z"
    }
   },
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:07.417442Z",
     "start_time": "2020-04-27T07:51:05.576059Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train for bagging classifier\n",
    "bag_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=500, \n",
    "                            max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out of Bag Error Estimation\n",
    "\n",
    "There is a very straight forward way to estimate the test error of a bagged model. In bagging models, some instances might be sampled multiple times while some instances might not be sampled at all. Generally, two-thirds of the samples are use for model training while the remaining one-third is not. They are referred to as the <u>out of bag</u> observations.\n",
    "\n",
    "We can these out of bag samples to be used to evalute our ensemble. This way, we can obtain an out-of-bag (OOB) MSE or accuracy score. This can be performed using SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:08.182083Z",
     "start_time": "2020-04-27T07:51:07.420527Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(ccp_alpha=0.0,\n",
       "                                                        class_weight=None,\n",
       "                                                        criterion='gini',\n",
       "                                                        max_depth=None,\n",
       "                                                        max_features=None,\n",
       "                                                        max_leaf_nodes=None,\n",
       "                                                        min_impurity_decrease=0.0,\n",
       "                                                        min_impurity_split=None,\n",
       "                                                        min_samples_leaf=1,\n",
       "                                                        min_samples_split=2,\n",
       "                                                        min_weight_fraction_leaf=0.0,\n",
       "                                                        presort='deprecated',\n",
       "                                                        random_state=None,\n",
       "                                                        splitter='best'),\n",
       "                  bootstrap=True, bootstrap_features=False, max_features=1.0,\n",
       "                  max_samples=100, n_estimators=500, n_jobs=-1, oob_score=True,\n",
       "                  random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train for bagging classifier, factoring in the OOB option\n",
    "bag_clf2 = BaggingClassifier(DecisionTreeClassifier(), \n",
    "                            n_estimators=500, max_samples=100, bootstrap=True, \n",
    "                            n_jobs=-1, oob_score=True)\n",
    "bag_clf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:08.191055Z",
     "start_time": "2020-04-27T07:51:08.184791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9253333333333333\n",
      "[[0.31315789 0.68684211]\n",
      " [0.38258575 0.61741425]\n",
      " [1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain the oob score\n",
    "print(bag_clf2.oob_score_)\n",
    "# Obtain the decision function for each OOB sample\n",
    "print(bag_clf2.oob_decision_function_[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:08.509157Z",
     "start_time": "2020-04-27T07:51:08.194099Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92\n"
     ]
    }
   ],
   "source": [
    "# Accuracy score for the test set\n",
    "y_pred2 = bag_clf2.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "To understand the model more, we can obtain the feature importance of each tree. For decision trees, simply use `tree.feature_importance_`. For the Bagging Classifier, recall that it consists of an ensemble of decision trees so we can also obtain `feature_importance_` for each tree and aggregate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:08.520530Z",
     "start_time": "2020-04-27T07:51:08.512657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52125406 0.47874594]\n",
      "[0.32160243 0.67839757]\n",
      "[0.45760872 0.54239128]\n"
     ]
    }
   ],
   "source": [
    "for i in bag_clf2.estimators_[:3]:\n",
    "    print(i.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:08.544288Z",
     "start_time": "2020-04-27T07:51:08.525056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56199095 0.43800905]\n"
     ]
    }
   ],
   "source": [
    "# Feature importance for Decision Tree example\n",
    "# Ingest\n",
    "iris_dataset = datasets.load_iris()\n",
    "X = pd.DataFrame(iris_dataset['data'], columns=iris_dataset['feature_names'])\n",
    "y = pd.Series(iris_dataset['target'])\n",
    "\n",
    "# Train\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X[['petal length (cm)','petal width (cm)']], y)\n",
    "\n",
    "# Feature importances for Decision trees\n",
    "print(tree_clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this in mind, we can combine Bagging classifiers with other regularisation parameters in traditional decision trees. For example, we can use the `max_features` parameter to train each tree only using a maximum number of randomly drawn subset of features from the original feature space. \n",
    "\n",
    "- Performing sampling on both features and instances during training is called <b>Random Patches</b>\n",
    "- Performing sampling only on the features is <b>Random Subspaces</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n",
    "\n",
    "Random Forests provide an improvement over bagged by way of a small tweak that decorrelates the trees. Similar to Bagging, Random Forests build a number of decision trees on bootstrapped training samples. But when building a tree, a random sample of $m$ predictors is chosen as split candidates from all $p$ features. A fresh sample of $m$ predictors is used when each new tree is built.\n",
    "\n",
    "This results in decorrelating the trees. Consider a dataset with one strong predictor, $p_1$. If bagging was used, then most if not all the trees will look quite similar to each other (seeing $p_1$ at the top split). Many of the bagged trees will be highly correlated, but averaging many highly correlated quantities <u>does not lead to a reduction in variance as compared to averaging many uncorrelated quantities</u>. So bagging does not lead to a substantial reduction in variance.\n",
    "\n",
    "Random forests overcome this by forcing each split to only consider a subset of features. Instead of searching for the best feature to split a node, it searches for the best feature among a random subset of features. On average, $\\frac{p-m}{p}$ of the splits will not have the strong predictor, and other predictors have a better chance of ending up in the top split. This process is <u>decorrelating the trees, resulting in the prediction to have less variance</u>. Some textbooks consider this a better diversity of trees. Of course, if $m=p$ then this process is equivalent to bagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:09.673689Z",
     "start_time": "2020-04-27T07:51:08.547436Z"
    }
   },
   "outputs": [],
   "source": [
    "forest_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "forest_clf.fit(X_train, y_train)\n",
    "y_pred3 = forest_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important features are likely to appear closer to the root of the tree. It is possible to see this by computing the average depth that the feature appears in all the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:09.786195Z",
     "start_time": "2020-04-27T07:51:09.676808Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.41475403, 0.58524597])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Trees \n",
    "\n",
    "Extra trees further enhances the Random forest by searching random thresholds for each feature rather than searching for the best threshold to split on. An ensemble built this way is an Extremely Randomised Trees ensemble (or Extra Trees ensemble). \n",
    "\n",
    "Compared to random forests, this is also much faster to train as finding the best threshold is the most expensive step of growing a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-27T07:51:10.686266Z",
     "start_time": "2020-04-27T07:51:09.788973Z"
    }
   },
   "outputs": [],
   "source": [
    "xtrees_clf = ExtraTreesClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "xtrees_clf.fit(X_train, y_train)\n",
    "y_pred4 = xtrees_clf.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
